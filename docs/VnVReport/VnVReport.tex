\documentclass[12pt, titlepage]{article}

\usepackage[a4paper, total={6in, 8in}]{geometry}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
% \usepackage[round]{natbib}
\usepackage[numbers,square]{natbib}

\usepackage{graphicx}
\graphicspath{ {./images/} }

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2023/03/26 & 0.1.0 & Creation\\
2023/04/02 & 0.1.1 & Start requirements testing sections\\
2023/04/03 & 0.2.0 & Fill in the Functional Requirements Evaluation
  section, along with test images\\
2023/04/02 & 0.3.0 & Complete NFR testing sections\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  GUI & Graphical User Interface\\
  MG & Module Guide\\
  MIS & Module Interface Specification\\
  NFR & Non Functional Requirement\\
  R & Requirement\\
  SRS & Software Requirements Specification\\
  T & Test\\
  VnV & Verification and Validation\\
  \bottomrule
\end{tabular}\\

~\newline
\noindent See the SRS \cite{SRS}, VnV Plan \cite{VnV_plan}, MG \cite{MG},
and MIS \cite{MIS} Documentation for additional items.

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

\section{Report Purpose}
This purpose of this report is to document the tasks accomplished and
testing results as part the verification and validation process of \progname{}
as laid out in the VnV Plan \cite{VnV_plan}. The code documentation along with notes
on developer setup and testing is available at:
\url{https://joedf.github.io/ImgBeamer/jsdocs/index.html} The software design documentation
is available at: \url{https://github.com/joedf/CAS741_w23}. The source code is
available at: \url{https://github.com/joedf/ImgBeamer/tree/cas741}

\section{Functional Requirements Evaluation}
In this section, we report the measures that were taken to evaluate whether the
functional requirements (as listed in the SRS \cite{SRS}) were met.
Most of these tests were executed manually due to the complex nature of visual information, GUI testing, and
the subjectivity of image quality.

\subsection{Image Import and Export (R1 and R6)}
This section is focused on testing the image import and export functionalities.

\subsubsection{T1: Test import for PNG/JPG/BMP format (R1)}
The software could successfully load or import valid (non-corrupt) images files in the PNG, JPG, and BMP formats.
An example is shown in figure \ref{fig_t1}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.5\textwidth]{t1.png}
  \caption{An example loaded test image.}
  \label{fig_t1}
  \end{center}
\end{figure}


\subsubsection{T2: Test PNG Image Export (R6)}
The software could successfully export valid PNG image files of the resulting image (see figure \ref{fig_t2}).
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.5\textwidth]{t2.png}
  \caption{The image was faithfully reproduced (in grayscale with some imaging parameters change for clarity).}
  \label{fig_t2}
  \end{center}
\end{figure}

\subsection{Spot Profile and Imaging Parameters (R2, R3, R4, and R5)}
This section focuses on testing the sampling and image rendering based on the given
imaging parameters and subregion / ROI. The ground truth / input image used for these tests
is depicted in figure \ref{fig_gt0} with the cyan overlay depicting the subregion area.

\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.5\textwidth]{gt0.png}
  \caption{The ground truth image used for testing with the subregion set to cover the entire image.}
  \label{fig_gt0} 
  \end{center}
\end{figure}


\subsubsection{T3: Spot Width and Height - Exact-sampling (R2 and R5)}
The test was passed as shown in figure \ref{fig_t3}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t3a.png}
   \includegraphics[width=5cm]{t3b.png}
  \caption{The spot layout and resulting image matching the expected test result:
  Spot size of 100 by 100\%.}
  \label{fig_t3} 
  \end{center}
\end{figure}

\subsubsection{T4: Spot Width and Height - Under-sampling (R2 and R5)}
The test was passed as shown in figure \ref{fig_t4}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t4a.png}
   \includegraphics[width=5cm]{t4b.png}
  \caption{The spot layout and resulting image matching the expected test result:
  Spot size of 10 by 10\%.}
  \label{fig_t4} 
  \end{center}
\end{figure}

\subsubsection{T5: Spot Width and Height - Over-sampling (R2 and R5)}
The test was passed as shown in figure \ref{fig_t5}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t5a.png}
   \includegraphics[width=5cm]{t5b.png}
  \caption{The spot layout and resulting image matching the expected test result:
  Spot size of 500 by 500\%.}
  \label{fig_t5} 
  \end{center}
\end{figure}

\subsubsection{T6: Spot Rotation - Astigmatism (R2 and R5)}
The test was passed as shown in figure \ref{fig_t6}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t6a.png}
   \includegraphics[width=5cm]{t6b.png}
  \caption{The spot layout and resulting image matching the expected test result:
  Spot size of 60 by 500\% at 45 degrees rotation.}
  \label{fig_t6} 
  \end{center}
\end{figure}

\subsubsection{T7: Raster Grid / Pixel Size (R3 and R5)}
The test was passed as shown in figure \ref{fig_t7}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t7a.png}
   \includegraphics[width=5cm]{t7b.png}
  \caption{The spot layout and resulting image matching the expected test result: an 8 by 8 pixel image.}
  \label{fig_t7} 
  \end{center}
\end{figure}

\subsubsection{T8: Subregion / ROI (R4 and R5)}
The test was passed as shown in figure \ref{fig_t8}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t8a.png}
   \includegraphics[width=5cm]{t8b.png}
   \includegraphics[width=5cm]{t8c.png}
   \includegraphics[width=5cm]{t8d.png}
   \includegraphics[width=5cm]{t8e.png}
  \caption{The spot layout, subregion, and resulting image matching the expected test result.}
  \label{fig_t8} 
  \end{center}
\end{figure}

\subsubsection{T9: Ground Truth Reproduction (R1, R2, R3, and R6)}
The test was passed as shown in figure \ref{fig_t9}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t9a.png}
   \includegraphics[width=5cm]{t9b.png}
  \caption{The expected test result: the ground image and resulting image are visually identical.}
  \label{fig_t9} 
  \end{center}
\end{figure}


\subsection{Image Quality Metric (R7)}
This section focuses on testing the image quality metric general cases. Naturally,
this is no flawless or foolproof image quality metric. Over 20 different image metrics have been
reviewed and compared by Jagalingam and Hegde in a 2015 paper,
each with their different strengths and weaknesses \cite{JAGALINGAM2015133}.

\subsubsection{T10: High metric value (approximate to exact-sampling)}
The test was passed as shown in figure \ref{fig_t10}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=4cm]{t10a.png}
   \includegraphics[width=4cm]{t10b.png}
   \includegraphics[width=4cm]{t10c.png}
   \includegraphics[width=4cm]{t10d.png}
   \includegraphics[width=4cm]{t10e.png}
   \includegraphics[width=4cm]{t10f.png}
  \caption{The spot layout, spot profile, subregion, and resulting image with a score equal or greater to
  the expected test result of a ``0.8501'' minimum.}
  \label{fig_t10} 
  \end{center}
\end{figure}

\subsubsection{T11: Low metric value (under-sampling)}
The test was passed as shown in figure \ref{fig_t11}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=4cm]{t10a.png}
   \includegraphics[width=4cm]{t10b.png}
   \includegraphics[width=4cm]{t11c.png}
   \includegraphics[width=4cm]{t11d.png}
   \includegraphics[width=4cm]{t11e.png}
   \includegraphics[width=4cm]{t11f.png}
  \caption{The spot layout, spot profile, subregion, and resulting image with a score less or equal to
  the expected test result of a ``0.8501'' maximum.}
  \label{fig_t11} 
  \end{center}
\end{figure}

\subsubsection{T12: Low metric value (over-sampling)}
The test was passed as shown in figure \ref{fig_t12}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=4cm]{t10a.png}
   \includegraphics[width=4cm]{t10b.png}
   \includegraphics[width=4cm]{t12c.png}
   \includegraphics[width=4cm]{t12d.png}
   \includegraphics[width=4cm]{t12e.png}
   \includegraphics[width=4cm]{t12f.png}
  \caption{The spot layout, spot profile, subregion, and resulting image with a score less or equal to
  the expected test result of a ``0.8501'' maximum.}
  \label{fig_t12} 
  \end{center}
\end{figure}

\subsubsection{T13: Control metric value} \label{T13}
The test was passed as shown in figure \ref{fig_t12}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t13a.png}
   \includegraphics[width=5cm]{t13b.png}
   \includegraphics[width=5cm]{t13c.png}
   \includegraphics[width=5cm]{t13d.png}
  \caption{The spot layout, spot profile, subregion, and resulting image with a score greater or equal to
  the expected test result of a ``0.9500'' minimum.}
  \label{fig_t12} 
  \end{center}
\end{figure}




\newpage
\clearpage

\section{Nonfunctional Requirements Evaluation}
This section focuses on the testing results verifying whether the nonfunctional requirements
(as defined in the SRS \cite{SRS}) are met. Emphasis is put on the \textit{usability} and
by extension \textit{portability}: The software should be easy to set up and use without
having to worry about any technicalities that could otherwise hinder or completely
prohibit non "tech-savvy" individuals from using the software.

\subsection{Usability}
\subsubsection{T14: Usability Survey (NFR2)}
The usability survey (see VnV Plan \cite{VnV_plan}) was not completed.
The software was casually reviewed in verbal discussion
with the expert consultants (as listed in the VnV Plan \cite{VnV_plan})
throughout the development of the software. Suggested features and minor issues has been
implemented into the software such as: the ability to set a spot size by numeric input,
draw the resulting row-by-row for responsiveness, and a display for spot shape eccentricity.

\subsection{Accuracy}
\subsubsection{T15: Image Metric Survey (NFR1)}
The image quality survey (see VnV Plan \cite{VnV_plan}) was not executed, but shall be some time in the future.

\subsection{Maintainability}
\subsubsection{T16: Static code analysis (NFR3)}
ESList was used to analyze the JavaScript code, and flake8 to analyze the Python code (only the
test code involves Python). ESLint reported 186 errors and 20 warnings. Flake8 reported
10 issues. All of which have been resolved. The code linters now return zero issues.

\subsubsection{T17: Code Review (NFR3)}
A code review was executed by the author, but should be done by another individual some time in the future.
Nearly all code that did not respect the code checklist (from the VnV Plan \cite{VnV_plan})
was rectified. In a few cases, some code has been marked for change with comments prefixed with
\code{TODO:}. Otherwise, all functions except for a few non-design-dependent utility functions
have been documented with \href{https://github.com/jsdoc/jsdoc}{JsDoc} style comments.
As a result, code documentation has been generated and is published at:
\url{https://joedf.github.io/ImgBeamer/jsdocs/index.html}

\subsection{Portability}
\subsubsection{T18: Various Platforms and Environments (NFR4)}
This test was executed by the author and was also indirectly tested successfully a number of times
by the expert consultants \cite{VnV_plan}. Users simply need to navigate to the following link
in any modern web browser to run and test the software:
\url{https://joedf.github.io/ImgBeamer/app/index.html}
	
\section{Comparison to Existing Implementation}	

This section is not applicable as an existing implementation for public use was not found or
is not available for comparison.

\section{Unit Testing}
As stated, unit testing was limited to image metrics (as explained in the VnV Plan \cite{VnV_plan}).
Unit tests for the image metrics are available under the \code{tests/image\_metrics} folder.

\subsection{Testing Setup}
The following are general instructions (repeated here for convenience to the reader)
to run the tests (each in their own subfolder).
\begin{itemize}
  \item \code{js-tests/}: for javascript implementations
  \begin{itemize}
    \item open the \code{index.html} page (using a local web server as described above)
      and look in the webconsole.
    \item Optionally, an online hosted version exists
    \href{https://joedf.github.io/ImgBeamer/tests/image_metrics/js-tests/}{here}.
    \item mainly in the webconsole, you can use \code{run\_all(fant)} where if \code{fant}
      is true, all the image comparison tests will be run using the ``fant-sampled'' image \cite{fant_1986} as
      the ground truth. Otherwise (false), it will use the ``original'' image as the ground truth instead.
  \end{itemize}
  \item \code{py-tests/}: for python implementations
  \begin{itemize}
    \item Install Python v3.10.6 or better (has not been tested on other versions)
    \item You'll likely need to run \code{pip install sewar} \textbf{once} to get the required
      image metrics module/library.
    \item run \code{imgquality.py}
  \end{itemize}
\end{itemize}

\subsection{Results for Functional Requirements}
\subsubsection{Control Unit Tests}
...

\subsection{Results for Nonfunctional Requirements}
\subsubsection{Control Unit Tests}
...

\section{Changes Due to Testing}
\begin{enumerate}
  \item The passing value for T13 (see \ref{T13}) had to be changed from ``0.9800'' to `0.9500' since the
    resulting images look identical. The resulting value of `0.9617' seemed satisfactory and the original
    expected test value was perhaps too constrained.
  \item Code is cleaner/clearer and better commentated/documented after running code linting and
    doing code review. Many unnecessary global variables have been eliminated.
\end{enumerate}


\section{Automated Testing}
Currently, this is no automated execution of the tests every time there is change.
That said, the tests are easy to run (just executing two files) and could be set up with
continuous integration some time in the future.

\section{Trace to Requirements}
		
\section{Trace to Modules}		

\section{Code Coverage Metrics}
Code coverage was used as part of this project. However, callgraphs had been generated using
\href{https://github.com/scottrogowski/code2flow}{Code2Flow}
to verify the flow application with respect to the design (as per the MG \cite{MG} and MIS \cite{MIS}).
An up-to-date but simplified (with limited search depth) callgraph is shown in figure \ref{fig_callgraph_simple}.
An older (somewhat out-dated) callgraph but with greater detail is shown in figure \ref{fig_callgraph_old_full}.

\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.8\textwidth]{callgraph/simple.png}
  \caption{An up-to-date callgraph generated using \href{https://github.com/scottrogowski/code2flow}{Code2Flow}}
  \label{fig_callgraph_simple} 
  \end{center}
\end{figure}

\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.8\textwidth]{callgraph/old-complex.png}
  \caption{A somewhat out-dated but more detailed callgraph generated using \href{https://github.com/scottrogowski/code2flow}{Code2Flow}}
  \label{fig_callgraph_old_full} 
  \end{center}
\end{figure}

\newpage
\clearpage

\bibliographystyle{plainnat}
\bibliography{../../refs/References,../../refs/cas741,../../refs/algorithms}


\end{document}