\documentclass[12pt, titlepage]{article}

\usepackage[a4paper, total={6in, 8in}]{geometry}

\usepackage{amsmath, mathtools}
\usepackage{siunitx}
\usepackage{pdflscape}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
% \usepackage[round]{natbib}
\usepackage[numbers,square]{natbib}

\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{xr}
\externaldocument{../SRS/SRS}
\externaldocument{../Design/MG/MG}
\externaldocument{../VnVPlan/VnVPlan}

\newcommand{\rref}[1]{R\ref{#1}}
\newcommand{\nfrref}[1]{NFR\ref{#1}}
\newcommand{\mref}[1]{M\ref{#1}}
\newcommand{\lref}[1]{L\ref{#1}}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2023/03/26 & 0.1.0 & Creation\\
2023/04/02 & 0.1.1 & Start requirements testing sections\\
2023/04/03 & 0.2.0 & Fill in the Functional Requirements Evaluation
  section, along with test images\\
2023/04/04 & 0.3.0 & Complete NFR testing sections\\
           & 0.3.1 & Start unit testing section\\
2023/04/05 & 0.4.0 & Complete first revision\\
           & 0.4.1 & Minor tweaks\\
           & 0.4.2 & Use references for code libraries\\
           & 0.4.3 & Added rank graphs\\
2023/04/06 & 0.4.4 & Added another change/remark from testing\\
2023/04/16 & 0.4.5 & Tweaks and fix some typos\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  GUI & Graphical User Interface\\
  L & Library\\
  MG & Module Guide\\
  MIS & Module Interface Specification\\
  NaN & Not a Number\\
  NFR & Non Functional Requirement\\
  R & Requirement\\
  SRS & Software Requirements Specification\\
  T & Test\\
  VnV & Verification and Validation\\
  \bottomrule
\end{tabular}\\

~\newline
\noindent See the SRS \cite{SRS}, VnV Plan \cite{VnV_plan}, MG \cite{MG},
and MIS \cite{MIS} Documentation for additional items.

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

\section{Report Purpose}
This purpose of this report is to document the tasks accomplished and
testing results as part the verification and validation process of \progname{}
as laid out in the VnV Plan \cite{VnV_plan}. The code documentation along with notes
on developer setup and testing is available at:
\url{https://joedf.github.io/ImgBeamer/jsdocs/index.html} The software design documentation
is available at: \url{https://github.com/joedf/CAS741_w23}. The source code is
available at: \url{https://github.com/joedf/ImgBeamer/tree/cas741}

\section{Functional Requirements Evaluation}
In this section, we report the measures that were taken to evaluate whether the
functional requirements (as listed in the SRS \cite{SRS}) were met.
Most of these tests were executed manually due to the complex nature of visual information, GUI testing, and
the subjectivity of image quality.

\subsection{Image Import and Export (R1 and R6)} \label{ss_inout}
This section is focused on testing the image import and export functionalities.

\subsubsection{T1: Test import for PNG/JPG/BMP format (R1)}
The software could successfully load or import valid (non-corrupt) images files in the PNG, JPG, and BMP formats.
An example is shown in figure \ref{fig_t1}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.5\textwidth]{t1.png}
  \caption{An example loaded test image.}
  \label{fig_t1}
  \end{center}
\end{figure}


\subsubsection{T2: Test PNG Image Export (R6)}
The software could successfully export valid PNG image files of the resulting image (see figure \ref{fig_t2}).
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.5\textwidth]{t2.png}
  \caption{The image was faithfully reproduced (in grayscale with some imaging parameters change for clarity).}
  \label{fig_t2}
  \end{center}
\end{figure}

\subsection{Spot Profile and Imaging Parameters (R2, R3, R4, and R5)} \label{ss_params}
This section focuses on testing the sampling and image rendering based on the given
imaging parameters and subregion / ROI. The ground truth / input image used for these tests
is depicted in figure \ref{fig_gt0} with the cyan overlay depicting the subregion area.

\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.5\textwidth]{gt0.png}
  \caption{The ground truth image used for testing with the subregion set to cover the entire image.}
  \label{fig_gt0} 
  \end{center}
\end{figure}


\subsubsection{T3: Spot Width and Height - Exact-sampling (R2 and R5)}
The test was passed as shown in figure \ref{fig_t3}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t3a.png}
   \includegraphics[width=5cm]{t3b.png}
  \caption{The spot layout and resulting image matching the expected test result:
  Spot size of 100 by 100\%.}
  \label{fig_t3} 
  \end{center}
\end{figure}

\subsubsection{T4: Spot Width and Height - Under-sampling (R2 and R5)}
The test was passed as shown in figure \ref{fig_t4}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t4a.png}
   \includegraphics[width=5cm]{t4b.png}
  \caption{The spot layout and resulting image matching the expected test result:
  Spot size of 10 by 10\%.}
  \label{fig_t4} 
  \end{center}
\end{figure}

\subsubsection{T5: Spot Width and Height - Over-sampling (R2 and R5)}
The test was passed as shown in figure \ref{fig_t5}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t5a.png}
   \includegraphics[width=5cm]{t5b.png}
  \caption{The spot layout and resulting image matching the expected test result:
  Spot size of 500 by 500\%.}
  \label{fig_t5} 
  \end{center}
\end{figure}

\subsubsection{T6: Spot Rotation - Astigmatism (R2 and R5)}
The test was passed as shown in figure \ref{fig_t6}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t6a.png}
   \includegraphics[width=5cm]{t6b.png}
  \caption{The spot layout and resulting image matching the expected test result:
  Spot size of 60 by 500\% at 45 degrees rotation.}
  \label{fig_t6} 
  \end{center}
\end{figure}

\subsubsection{T7: Raster Grid / Pixel Size (R3 and R5)}
The test was passed as shown in figure \ref{fig_t7}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t7a.png}
   \includegraphics[width=5cm]{t7b.png}
  \caption{The spot layout and resulting image matching the expected test result: an 8 by 8 pixel image.}
  \label{fig_t7} 
  \end{center}
\end{figure}

\subsubsection{T8: Subregion / ROI (R4 and R5)}
The test was passed as shown in figure \ref{fig_t8}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t8a.png}
   \includegraphics[width=5cm]{t8b.png}
   \includegraphics[width=5cm]{t8c.png}
   \includegraphics[width=5cm]{t8d.png}
   \includegraphics[width=5cm]{t8e.png}
  \caption{The spot layout, subregion, and resulting image matching the expected test result.}
  \label{fig_t8} 
  \end{center}
\end{figure}

\subsubsection{T9: Ground Truth Reproduction (R1, R2, R3, and R6)}
The test was passed as shown in figure \ref{fig_t9}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t9a.png}
   \includegraphics[width=5cm]{t9b.png}
  \caption{The expected test result: the ground image and resulting image are visually identical.}
  \label{fig_t9} 
  \end{center}
\end{figure}


\subsection{Image Quality Metric (R7)} \label{ss_metrics}
This section focuses on testing the image quality metric general cases. Naturally,
this is no flawless or foolproof image quality metric. Over 20 different image metrics have been
reviewed and compared by Jagalingam and Hegde in a 2015 paper,
each with their different strengths and weaknesses \cite{JAGALINGAM2015133}.

\subsubsection{T10: High metric value (approximate to exact-sampling)}
The test was passed as shown in figure \ref{fig_t10}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=4cm]{t10a.png}
   \includegraphics[width=4cm]{t10b.png}
   \includegraphics[width=4cm]{t10c.png}
   \includegraphics[width=4cm]{t10d.png}
   \includegraphics[width=4cm]{t10e.png}
   \includegraphics[width=4cm]{t10f.png}
  \caption{The spot layout, spot profile, subregion, and resulting image with a score equal or greater to
  the expected test result of a ``0.8501'' minimum.}
  \label{fig_t10} 
  \end{center}
\end{figure}

\subsubsection{T11: Low metric value (under-sampling)}
The test was passed as shown in figure \ref{fig_t11}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=4cm]{t10a.png}
   \includegraphics[width=4cm]{t10b.png}
   \includegraphics[width=4cm]{t11c.png}
   \includegraphics[width=4cm]{t11d.png}
   \includegraphics[width=4cm]{t11e.png}
   \includegraphics[width=4cm]{t11f.png}
  \caption{The spot layout, spot profile, subregion, and resulting image with a score less or equal to
  the expected test result of a ``0.8501'' maximum.}
  \label{fig_t11} 
  \end{center}
\end{figure}

\subsubsection{T12: Low metric value (over-sampling)}
The test was passed as shown in figure \ref{fig_t12}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=4cm]{t10a.png}
   \includegraphics[width=4cm]{t10b.png}
   \includegraphics[width=4cm]{t12c.png}
   \includegraphics[width=4cm]{t12d.png}
   \includegraphics[width=4cm]{t12e.png}
   \includegraphics[width=4cm]{t12f.png}
  \caption{The spot layout, spot profile, subregion, and resulting image with a score less or equal to
  the expected test result of a ``0.8501'' maximum.}
  \label{fig_t12} 
  \end{center}
\end{figure}

\subsubsection{T13: Control metric value} \label{T13}
The test was passed as shown in figure \ref{fig_t12}.
\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=5cm]{t13a.png}
   \includegraphics[width=5cm]{t13b.png}
   \includegraphics[width=5cm]{t13c.png}
   \includegraphics[width=5cm]{t13d.png}
  \caption{The spot layout, spot profile, subregion, and resulting image with a score greater or equal to
  the expected test result of a ``0.9500'' minimum.}
  \label{fig_t12} 
  \end{center}
\end{figure}




\newpage
\clearpage

\section{Nonfunctional Requirements Evaluation}
This section focuses on the testing results verifying whether the nonfunctional requirements
(as defined in the SRS \cite{SRS}) are met. Emphasis is put on the \textit{usability} and
by extension \textit{portability}: The software should be easy to set up and use without
having to worry about any technicalities that could otherwise hinder or completely
prohibit non "tech-savvy" individuals from using the software.

\subsection{Usability} \label{ss_usability}
\subsubsection{T14: Usability Survey (NFR2)}
The usability survey (see VnV Plan \cite{VnV_plan}) was not completed.
The software was casually reviewed in verbal discussion
with the expert consultants (as listed in the VnV Plan \cite{VnV_plan})
throughout the development of the software. Suggested features and minor issues has been
implemented into the software such as: the ability to set a spot size by numeric input,
draw the resulting row-by-row for responsiveness, and a display for spot shape eccentricity.

\subsection{Accuracy} \label{ss_accuracy}
\subsubsection{T15: Image Metric Survey (NFR1)}
The image quality survey (see VnV Plan \cite{VnV_plan}) was not executed, but shall be some time in the future.

\subsection{Maintainability} \label{ss_maintain}
\subsubsection{T16: Static code analysis (NFR3)}
ESList was used to analyze the JavaScript code, and flake8 to analyze the Python code (only the
test code involves Python). ESLint reported 186 errors and 20 warnings. Flake8 reported
10 issues. All of which have been resolved. The code linters now return zero issues.

\subsubsection{T17: Code Review (NFR3)}
A code review was executed by the author, but should be done by another individual some time in the future.
Nearly all code that did not respect the code checklist (from the VnV Plan \cite{VnV_plan})
was rectified. In a few cases, some code has been marked for change with comments prefixed with
\code{TODO:}. Otherwise, all functions except for a few non-design-dependent utility functions
have been documented with \href{https://github.com/jsdoc/jsdoc}{JsDoc} style comments.
As a result, code documentation has been generated and is published at:
\url{https://joedf.github.io/ImgBeamer/jsdocs/index.html}

\subsection{Portability} \label{ss_portability}
\subsubsection{T18: Various Platforms and Environments (NFR4)}
This test was executed by the author and was also indirectly tested successfully a number of times
by the expert consultants \cite{VnV_plan}. Users simply need to navigate to the following link
in any modern web browser to run and test the software:
\url{https://joedf.github.io/ImgBeamer/app/index.html}
	
\section{Comparison to Existing Implementation}	

This section is not applicable as an existing implementation or similar application
for public use was not found or is not available for comparison.

\section{Unit Testing} \label{ss_unittest}
As stated, unit testing was limited to image metrics (as explained in the VnV Plan \cite{VnV_plan}).
Unit tests for the image metrics are available under the \code{tests/image\_metrics} folder.

Some statistics on the unit tests:
\begin{itemize}
  \item A total of 247 tests (13 metric variants with 19 images comparisons each) were
    executed in 931 \si{ms} for the image metrics implemented in JavaScript.
  \item A total of 152 tests (8 metric variants with 19 images comparisons each) were
    executed in 38.3125 \si{s} for the image metrics implemented in Python.
\end{itemize}


\subsection{Testing Setup}
The following are general instructions (repeated here for convenience to the reader)
to run the tests (each in their own subfolder).
\begin{itemize}
  \item \code{js-tests/}: for javascript implementations
  \begin{itemize}
    \item open the \code{index.html} page (using a local web server as described above)
      and look in the webconsole.
    \item Optionally, an online hosted version exists
    \href{https://joedf.github.io/ImgBeamer/tests/image_metrics/js-tests/}{here}.
    \item mainly in the webconsole, you can use \code{run\_all(fant)} where if \code{fant}
      is true, all the image comparison tests will be run using the ``fant-sampled'' image \cite{fant_1986} as
      the ground truth. Otherwise (false), it will use the ``original'' image as the ground truth instead.
  \end{itemize}
  \item \code{py-tests/}: for python implementations
  \begin{itemize}
    \item Install Python v3.10.6 or better (has not been tested on other versions)
    \item You'll likely need to run \code{pip install sewar} \textbf{once} to get the required
      image metrics module/library.
    \item run \code{imgquality.py}
  \end{itemize}
\end{itemize}

\subsection{Results for Functional Requirements}
\subsubsection{Image Metric Control Testing}
All these tests passed within the margin of error.

\subsection{Results for Nonfunctional Requirements}
The tests are experimental in nature to help determine if there is a more fitting
metric to use. Failures here do not mean a failure for the software.
Failures found here may represent a misunderstanding by the author or a limitation of the metrics.
All tests passed unless otherwise stated below.

\subsubsection{Image Metric Control Testing}
Please refer to figures \ref{fig_js_results} and \ref{fig_py_results} for the results.
\begin{itemize}
  \item All metrics passed U10 variants U1 to U3,
    except for ``psnr (darosh)'' (from \lref{lib_mse}) and ``mssim (sewar)'' (from \lref{lib_sewar})
    which both failed U10 (variant U3).
  \item Nearly all metrics failed U10 variants U4 and U5,
    except for ``(i)nrmse'' (from \lref{lib_prog}) and ``mssim (darosh)'' (from \lref{lib_msssim}).
    That said, they all scored consistently (about equal).
    The MSE-based metric that failed were due to a different
    normalization step.
  \item Nearly all metrics failed U10 variants U6 and U7,
    except for the metric used by \progname{} (\ref{lib_prog}).
    MSE-based metrics all scored consistently. The SSIM-based and UQI
    metrics were not consistent at all, and seems to representing
    a limitation, design flaw, or an inappropriate test
    (likely was designed for different image types).
  \item For U10 variants U8, same as above.
  \item Other than the first two u10 test variants (U1 and U2),
    the SCC and VIFP metrics scores were all failures:
    all 0 or NaN (invalid, calculation could not be completed).
\end{itemize}


\subsubsection{Image Metric Experimental Testing}
The values (figures \ref{fig_js_results} and \ref{fig_py_results}) were sorted into
rank orders which be can viewed in figures \ref{fig_js_ranks}, \ref{fig_py_ranks},
\ref{fig_js_graph}, and \ref{fig_py_graph}.
None of the metrics fit the expected rank order perfectly. That said,
The MSE-based metrics, followed closely by UIQ then SSIM-based, fit closely; the discrepancies
were all based on values that were within $\pm$ 0.02 of each other. These are otherwise considered
relatively good fits. All other metrics did not fit the expected order at all.


\begin{landscape}
\vspace*{\fill}
\begin{figure}[h!]
  \begin{center}
  \includegraphics[width=22cm]{test-analysis/js-tests-results.png}
  \caption{The results (metric values) from the JavaScript tests.
  As indicated in the VnV Plan \cite{VnV_plan}:
  ``ssim (obartra)'' refers to \lref{lib_ssim}, ``ssim (darosh)'' refers to \lref{lib_ssim1},
  columns with ``jdf'' are metrics implemented by the author as \lref{lib_prog},
  ``mse and psnr (darosh)'' refer to \lref{lib_mse}, and
  ``msssim and ssim (darosh)'' refer to \lref{lib_msssim}.}
  \label{fig_js_results}
  \end{center}
\end{figure}
\vspace*{\fill}
\end{landscape}

\begin{figure}[h!]
  \begin{center}
  \includegraphics[width=1.0\textwidth]{test-analysis/py-tests-results.png}
  \caption{The results (metric values) from the Python tests.}
  \label{fig_py_results}
  \end{center}
\end{figure}


\begin{figure}[h!]
  \begin{center}
  \includegraphics[width=0.95\textwidth]{test-analysis/js-ranked.png}
  \caption{The ranked results (metric values) from the JavaScript tests.
  Red highlighted values are mismatches compared to the expected order.}
  \label{fig_js_ranks}
  \end{center}
\end{figure}

\begin{figure}[h!]
  \begin{center}
  \includegraphics[width=0.7\textwidth]{test-analysis/py-ranked.png}
  \caption{The ranked results (metric values) from the Python tests.
  Red highlighted values are mismatches compared to the expected order.}
  \label{fig_py_ranks}
  \end{center}
\end{figure}


\begin{figure}[h!]
  \begin{center}
  \includegraphics[width=0.7\textwidth]{test-analysis/js-graph.pdf}
  \caption{A graph of the JavaScript test results (metric values) of the normalized
  image metrics (from \lref{lib_prog}, \lref{lib_ssim},
  \lref{lib_ssim1} and \lref{lib_msssim}) versus the expected rank order.}
  \label{fig_js_graph}
  \end{center}
\end{figure}

\begin{figure}[h!]
  \begin{center}
  \includegraphics[width=0.7\textwidth]{test-analysis/py-graph.pdf}
  \caption{A graph of the Python test results (metric values) of the normalized
  image metrics (from \lref{lib_sewar}) versus the expected rank order.}
  \label{fig_py_graph}
  \end{center}
\end{figure}


\newpage
\clearpage

\section{Remarks and Changes Due to Testing}
\begin{enumerate}
  \item The passing value for T13 (see \ref{T13}) had to be changed from ``0.9800'' to `0.9500' since the
    resulting images look identical. The resulting value of `0.9617' seemed satisfactory and the original
    expected test value was perhaps too constrained.
  \item Through manual testing (mainly section \ref{ss_metrics})
    and watching for runtime warnings and errors,
    a small discrepancy was found in the sizes of the compared images with
    the image metrics: one image was 1-2 pixels taller depending on the size
    of the web browser window. This was due to sub-pixel position calculations
    of the canvas when the UI is first drawn. This has been rectified by
    ensuring all values are rounded up to the nearest integers.
  \item Manual testing (mainly section \ref{ss_metrics}) also
    revealed a limitation in the MSE-based metrics (as used by \progname{}):
    two images with very similar average gray values overall will
    erroneously score a "good" score (e.g., $\ge 80\%$). This is a design
    flaw or limitation likely documented in literature (but did not check),
    stemming from the use of averaging.
  \item For similar reasons, the error margin for the unit tests was adjusted from $\pm$ 2\% to $\pm$ 5\%.
  \item Code is cleaner/clearer and better commentated/documented after running code linting and
    doing code review. Many unnecessary global variables have been eliminated.
  \item Switching over to the UQI/UIQ or SSIM metrics may be worth considering.
  \item Noticed from unit testing that some image metrics in javascript (js-tests) were somewhat
    inconsistent with their python counterpart (py-tests) because of inaccurate pixel values being read. For example,
    black pixels (0x000000) where read as (0x010101) or gray pixels (0x808080) being
    read as (0x767676)... As a result, the output metric values were very liekly
    off because of this. An \href{https://github.com/joedf/ImgBeamer/issues/25}{issue}
    has been opened on the \href{https://github.com/joedf/ImgBeamer}{GitHub code repository}
    to explore any possible mitigations.
\end{enumerate}


\section{Automated Testing}
Currently, this is no automated execution of the tests every time there is change.
Automated testing using \href{https://pytest.org}{pytest} and \href{https://qunitjs.com}{QUnit}
to display and calculate the test pass percentages and execution times would be helpful
and is desired (perhaps sometime in the future),
but have not been set up at this time due to a lack of time / resources.
That said, the tests are easy to run (just executing two files)
and could be set up with continuous integration some time in the future.


\section{Trace to Requirements}
In Table \ref{Table:R_trace}, the connections between the test sections and the requirements are shown.

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
      \hline
      Section &
      \rref{R_Inputs} & \rref{R_userSpotProfile} & \rref{R_userPixelSize}
      & \rref{R_subregion} & \rref{R_spotLayout} & \rref{R_resultImage} &
      \rref{R_imageMetric} &
      \nfrref{NFR_Accuracy} &
      \nfrref{NFR_Usability} &
      \nfrref{NFR_Maintainability} &
      \nfrref{NFR_Portability}
      \\
      \hline
       \ref{ss_inout}        &X& & & & &X& & & & & \\ \hline
       \ref{ss_params}       & &X&X&X&X&X& & & & & \\ \hline
       \ref{ss_metrics}      & & & & & & &X& & & & \\ \hline
       \ref{ss_unittest}     & & & & & & &X& & & & \\ \hline
       \ref{ss_accuracy}     & & & & & & & &X& & & \\ \hline
       \ref{ss_usability}    & & & & & & & & &X& & \\ \hline
       \ref{ss_maintain}     & & & & & & & & & &X& \\ \hline
       \ref{ss_portability}  & & & & & & & & & & &X\\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between the Test Sections
    and the Requirements}
  \label{Table:R_trace}
\end{table}

\section{Trace to Modules}
In Tables \ref{Table:M_trace} and \ref{Table:M_trace2},
the connections between the test sections and the modules are shown.

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
      \hline
      Section &
      \mref{M_HdwHide} &
      \mref{M_control} &
      \mref{M_imgGTInput} &
      \mref{M_params} &
      \mref{M_inSpotProfile} &
      \mref{M_export} &
      \mref{M_infoDisp} &
      \mref{M_vizGT} &
      \mref{M_vizSubregion} &
      \mref{M_vizSpotProfile} &
      \mref{M_vizSpotContent} &
      \mref{M_vizSpotSignal}
      \\
      \hline
       \ref{ss_inout}     &X& &X& & &X& & & & &X&X\\ \hline
       \ref{ss_params}    &X& & &X&X& & &X&X&X&X&X\\ \hline
       \ref{ss_metrics}   & & & & & & &X& & & & & \\ \hline
       \ref{ss_unittest}  & & & & & & &X& & & & & \\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between the Test Sections
    and the Modules (Part 1)}
  \label{Table:M_trace}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
      \hline
      Sections &
      \mref{M_vizSpotLayout} &
      \mref{M_vizSampledSub} &
      \mref{M_vizResultSub} &
      \mref{M_vizResultImg} &
      \mref{M_dispControl} &
      \mref{M_GUI} &
      \mref{M_drawStage} &
      \mref{M_rendering} &
      \mref{M_metric}
      \\
      \hline
       \ref{ss_inout}     & &X&X&X&X&X&X&X& \\ \hline
       \ref{ss_params}    &X&X&X&X&X&X&X&X& \\ \hline
       \ref{ss_metrics}   & & & & & &X&X&X&X\\ \hline
       \ref{ss_unittest}  & & & & & &X&X&X&X\\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between the Test Sections
    and the Modules (Part 2)}
  \label{Table:M_trace2}
\end{table}


\section{Code Coverage Metrics}
Code coverage was not used as part of this project. However, callgraphs have been generated using
\href{https://github.com/scottrogowski/code2flow}{Code2Flow}
to verify the flow application with respect to the design (as per the MG \cite{MG} and MIS \cite{MIS}).
An up-to-date but simplified (with limited search depth) callgraph is shown in figure \ref{fig_callgraph_simple}.
An older (somewhat out-dated) callgraph but with greater detail is shown in figure \ref{fig_callgraph_old_full}.

\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.8\textwidth]{callgraph/simple.png}
  \caption{An up-to-date callgraph generated using \href{https://github.com/scottrogowski/code2flow}{Code2Flow}}
  \label{fig_callgraph_simple} 
  \end{center}
\end{figure}

\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.8\textwidth]{callgraph/old-complex.png}
  \caption{A somewhat out-dated but more detailed callgraph generated using \href{https://github.com/scottrogowski/code2flow}{Code2Flow}}
  \label{fig_callgraph_old_full} 
  \end{center}
\end{figure}

\newpage
\clearpage

\bibliographystyle{plainnat}
\bibliography{../../refs/References,../../refs/cas741,../../refs/algorithms}


\end{document}