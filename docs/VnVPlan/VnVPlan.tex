\documentclass[12pt, titlepage]{article}

\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
% \usepackage[round]{natbib}
\usepackage[numbers,square]{natbib}

\newcounter{testnum} %test Number
\newcommand{\dthetestnum}{T\thetestnum}
\newcommand{\tref}[1]{T\ref{#1}}

\newcounter{unittestnum} %unittest Number
\newcommand{\dtheunittestnum}{U\theunittestnum}
\newcommand{\utref}[1]{U\ref{#1}}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{2.2cm}p{1.8cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2023/02/17 & 1.0 & First version \\
2023/02/20 & 1.0.1 & fix minor issues in T\ref{T_exportImage}, T\ref{T_spotSize}, T\ref{T_linters}, 
  and T\ref{T_portability}\\
  & 1.0.1 & Minor issues in general information\\
  & 1.0.2 & Fix grammar in sections \ref{sec_srs_vplan}, 
  \ref{sec_design_vplan}, \ref{sec_impl_vplan}, and \ref{sec_autotest_tools}. \\
  & 1.0.3 & Fix formatting and grammar in sections \ref{sec_systest_desc}, 
  \ref{subsec_img_io}, \ref{subsec_spot}, and \ref{subsec_img_metric}. \\
  & 1.0.4 & Fix grammar in Section \ref{sec_NFR_tests}. \\
  & 1.0.5 & Tweaks to Section \ref{sec_objectives}. \\
  & 1.0.6 & Added missing references to MG and MIS. \\
2023/02/22 & 1.0.7 & Provide specific values for most of the tests. \\
  & 1.0.8 & Review the design and VnV verification plans. \\
2023/02/27 & 1.0.9 & Improve wording for Section \ref{sec_impl_vplan}. \\
  & 1.0.10 & Tweak tests in sections \ref{subsec_spot} and \ref{subsec_img_metric}. \\
2023/03/23 & Improve clarity for how some of the test will be performed.\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  MIS & Module Interface Specification \\
  MG & Module Guide \\
  MSE & Mean Squared Error\\
  RMSE & Root Mean Square Error\\
  NRMSE & Normalized Root Mean Square Error\\
  NFR & Nonfunctional Requirement\\
  GUI & Graphical User Interface\\
  R & Requirement\\
  ROI & Region of Interest\\
  SRS & Software Requirements Specification\\
  T & Test\\
  U & Unit test\\
  UI & User Interface\\
  VnV & Verification and Validation\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}

\section{General Information}

This document lays out the Verification and Validation (VnV) plan of \progname{}
as described by the Software Requirements Specification (SRS) 
\citep{SRS}. Testing of the software and its components will be conducted to build confidence in 
its usability, accuracy, and, ultimately, whether it meets the requirements from the SRS.

\subsection{Summary}

\progname{} software aims to be a demonstration and learning tool of how a scanning electron 
microscope (SEM) formulates an image. The idea is to help visualize the influence trends of imaging 
parameters (as defined in the SRS) on image quality (or clarity) and resolution.


\subsection{Objectives} \label{sec_objectives}

The objective is to produce a tool that is easy to use (\textit{usability}). It should feel intuitive 
to the user and should provide easy-to-understand information. This is so that the user may 
identify any trends in how the final image is affected with relative ease and confidence. As a 
secondary goal, the \textit{accuracy} in the trends are important. Due to the subjective and 
qualitative nature of evaluating image quality, the metric used should be straight-forward and 
provide some objectivity (for quantitative comparison) and assurance to the users.

\subsection{Relevant Documentation}

There are multiple design documents that provide the important and intimate details to understand 
some of the concepts that are being tested. These documents are the following:

\begin{itemize}
  \item Problem Statement \citep{Prob_Statement}
  \item SRS \citep{SRS}
  \item VnV Report \citep{VnV_report}
  \item MG \citep{MG}
  \item MIS \citep{MIS}
\end{itemize}

\section{Plan}

In this section, multiple plans are described to test and inspect the software with an emphasis 
on \textit{usability}. Multiple approaches and perspectives will be employed by the VnV team (\ref{sec_vnv_team})
to help build confidence in the requirements, to avoid any missed important details, 
and to deliver on the qualities as mentioned in the objective (\ref{sec_objectives}).

\subsection{Verification and Validation Team} \label{sec_vnv_team}

The members of the VnV team as well their individual roles are listed in the following table:

\begin{table}[h!]
  \centering
  \begin{tabular}{|r|l|}
    \rowcolor[gray]{0.9}
    \hline
    \textbf{Role} & \textbf{Name} \\ \hline
    Project Supervisor & Dr.\ Spencer Smith  \\ \hline
    Author             & \authname           \\ \hline
    Domain Expert      & Karen Wang          \\ \hline
    SRS Reviewer       & Jason Balaci        \\ \hline
    VnV Plan Reviewer  & Sam Crawford        \\ \hline
    MG + MIS Reviewer  & Lesley Wheat        \\ \hline
    Expert Consultant  & Dr. Nabil Bassim    \\ \hline
    Expert Consultant  & Michael W. Phaneuf  \\ \hline
  \end{tabular}
  \caption{Table of the VnV Team Members}
  \label{table_vnv_team}
\end{table}

\an{Dr. Bassim is my PhD Supervisor, and Mr. Phaneuf is my co-supervisor (employer/industry), 
since I am doing an industrial PhD. Much of this project was originally conceptualized with 
the help and guidance of Mr. Phaneuf.}


\subsection{SRS Verification Plan} \label{sec_srs_vplan}

The SRS will be reviewed by the project supervisor, the SRS reviewer and the author. Some input
may be given by the expert consultants if time permits. Most of the feedback has been provided 
as issues on GitHub, as annotated documents, or by verbal exchange. 
Throughout the development of the project, the author is expected 
to make changes needed to the resolve the issues. The reviewers may refer to the SRS checklist \citep{SRS_checklist}.
The key objective is to verify that the software requirements and the documentation are sound 
and coherent to the intended audience as defined in the SRS.


\subsection{Design Verification Plan} \label{sec_design_vplan}

Much of the conceptualization was done after having multiple discussions with the expert 
consultants and having done literature review for preexisting tools and documentation 
relevant to the project. Decisions were made with a focus on the usability, meaning little to 
no setup required. The design and implementation is documented in the 
MG\citep{MG}/MIS\citep{MIS}.
The VnV team, as well as any welcomed volunteers, will provide
their input (through GitHub issues). Eventually, the project may be published in a journal
where the software \progname~and its accompanying documentation will 
likely be rigorously reviewed; The expert consultants will review and evaluate the software to
establish whether it is fit for public use.

\an{Not sure what was appropriate to put here.}


\subsection{Verification and Validation Plan Verification Plan}

The goal is to uncover any mistakes and reveal any risks (such as misconceptions or coverage gaps) 
through the supervision and 
review of the VnV team members. Once most of the work has been done, the work and
accompanying documentation shall undergo a vetting process: the VnV team
will check whether the documented testing plans and verification process have been 
accomplished and the requirements fulfilled. The author will then review the documents once more
against the VnV checklist \citep{VnV_checklist}, before a final review by the rest of the VnV team.

\subsection{Implementation Verification Plan}  \label{sec_impl_vplan}

Much of the software will be tested manually by users. This will include checking for
any inconsistencies, bugs in the graphical user interface (GUI), and unexpected
artifacts in images produced.
The image metrics will be tested using unit tests (Section \ref{sec_unittest}). For the
all the code implemented, linters will be used as mentioned in Section \ref{sec_autotest_tools}.
As a control for the image metrics, they will be calculated using the ground truth image as both the 
input and the reference image (effectively comparing it to itself) 
to rule out any baseline or unexpected factors in the implementations themselves.


\subsection{Automated Testing and Verification Tools} \label{sec_autotest_tools}

The image quality metric shall be unit tested using \href{https://pytest.org}{pytest} for 
automated testing of any algorithms implemented in Python and \href{https://qunitjs.com}{QUnit} 
shall be used for those implemented in JavaScript. The unit tests are listed in 
Section \ref{sec_unittest}.
As for linters, \href{https://flake8.pycqa.org}{flake8} shall be used for Python code 
and \href{https://eslint.org}{ESLint} for JavaScript code.
The \href{https://github.com/andrewekhalel/sewar}{sewar} python package will be 
used as a reference implementation in Python for the image quality metrics.


\subsection{Software Validation Plan}
A \textit{usability} survey will be conducted to evaluate the user experience and whether 
the GUI is intuitive enough to the intended users as described in the SRS \citep{SRS}.
An \textit{accuracy} survey will be conducted to assess the user-perceived image quality. 
The trends identified in the surveys results will be compared to the calculated image metrics.
As a control for the images produced, a manual and an automated test will be conducted to verify if 
an image identical (or with unperceivable difference) to ground truth image can be reproduced.
The compared images shall be in the \textit{accuracy} survey for confirmation. This can be compared
as well using the image metrics. 

\an{I think I might be confusing the Software and the Implementation validation plans...}

\section{System Test Description} \label{sec_systest_desc}

In this section, the system tests that will be conducted are described in detail. These tests
will be used to verify the fulfillment of the requirements as listed in the SRS \citep{SRS}.
Most, if not all, of the tests listed here will be manually performed. Automatic
tests will be unit tests as described in Section \ref{sec_unittest}.

\subsection{Tests for Functional Requirements}

The tests present will verify whether the functional requirements 
(as listed in the SRS \cite{SRS}) are met and validate that the outputs
are as expected.

\subsubsection{Image Import and Export} \label{subsec_img_io}

To satisfy R1 from the SRS \citep{SRS}, an input image for the following formats listed below shall be 
accepted provided they follow the input data constraints.
Some preloaded example images shall be included and be available to the user for use as well.
As well as for R6, the software should allow for export of the resulting image.

\begin{itemize}
  \item{PNG}
  \item{JPG}
  \item{BMP\\}
\end{itemize}

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_inputImage}}: Test import for PNG/JPG/BMP format\\}
            
  Control: Manual
            
  Initial State: \progname{} loaded and idle.
            
  Input: A non-corrupt (valid) PNG, JPG, or BMP image file.
            
  Output: The image should be visible and be displayed as expected as the ground truth image.
            
  Test Case Derivation: These are some of the most common image file formats and should be compatible with the software.
            
  How test will be performed: The user will click the ``Load image'' button and select an image to import.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_exportImage}}: Test PNG Image Export\\}
              
  Control: Manual
            
  Initial State: \progname{} loaded and idle.
            
  Input: User clicks the \textit{Export} button.
            
  % Output: \wss{The expected result for the given inputs}
  Output: The output image file should look the same as what is displayed in the ``Resulting Image'' display.

  % Test Case Derivation: \wss{Justify the expected value given in the Output field}
  Test Case Derivation: \an{Not sure what this means, and how this differs from what's explained Output.}

  How test will be performed: The user will click the ``Export image'' button and choose where to save the image file.

\end{enumerate}


\subsubsection{Spot Profile and Imaging Parameters} \label{subsec_spot}

To satisfy R2 and R5 from the SRS \citep{SRS}, the software should accept input from the user
to change the width, height, and rotation which, define an ellipsoid shape. 
This shape is then used as the spot to sample the ground truth image.
To satisfy R3, the software should accept user input for a real positive number for the pixel size.
To satisfy R4, the user should be able to specify a subregion (or ROI) for processing.

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_spotSize}}: Spot Width and Height - Exact-sampling\\}

    Control: Manual
              
    Initial State: \progname{} loaded and idle.
              
    Input: Both width and height are set to ``100\%''.
              
    Output: The spot layout should reflect these changes and display an updated arrangement with the given shape.
    The resulting image should exhibit the expected \textbf{exact-sampling} case as documented by the SRS figures \citep{SRS}.

    Test Case Derivation: \an{not needed in this case?}
              
    How test will be performed: Manually, either by using the mouse scroll wheel in
      the spot content display, or by double-clicking on the width display and entering '100'
      in prompt that appears.
					
    \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_spotSize10}}: Spot Width and Height - Under-sampling\\}

    Control: Manual
              
    Initial State: \progname{} loaded and idle.
              
    Input: Both width and height are set to ``10\%''.
              
    Output: The spot layout should reflect these changes and display an updated arrangement with the given shape.
    The resulting image should exhibit the expected \textbf{under-sampling} case as documented by the SRS figures \citep{SRS}.

    Test Case Derivation: \an{not needed in this case?}
              
    How test will be performed: Manually, either by using the mouse scroll wheel in
      the spot content display, or by double-clicking on the width display and entering '10'
      in prompt that appears.
					
    \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_spotSize500}}: Spot Width and Height - Over-sampling\\}

    Control: Manual
              
    Initial State: \progname{} loaded and idle.
              
    Input: Both width and height are set to ``500\%''.
              
    Output: The spot layout should reflect these changes and display an updated arrangement with the given shape.
    The resulting image should exhibit the expected \textbf{over-sampling} case as documented by the SRS figures \citep{SRS}.

    Test Case Derivation: \an{not needed in this case?}
              
    How test will be performed: Manually, either by using the mouse scroll wheel in
      the spot content display, or by double-clicking on the width display and entering '500'
      in prompt that appears.
					
  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_spotRotation}}: Spot Rotation - Astigmatism\\}

    Control: Manual
                
    Initial State: \progname{} loaded and idle.
              
    Input: The angle is set to ``45 degrees'', the width is set ``60\%'', and the height is set to ``500\%''.
              
    Output: The spot layout should reflect the updated arrangement with the rotated spots.
    The resulting image should exhibit the expected \textbf{astigmatism} effects as documented in the SRS figures \citep{SRS}.

    Test Case Derivation: \an{see output?}

    How test will be performed: How test will be performed: Manually, either by using the mouse scroll wheel in
    the spot content display, or by double-clicking on the width display and entering '100'
    in prompt that appears. And, by dragging the rotation node in the spot profile display
    until an angle of 45 is reached.
					
    \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_rasterGrid}}: Raster Grid / Pixel Size\\}

    Control: Manual
                
    Initial State: \progname{} loaded and idle.
              
    Input: The rows and columns are set to ``8'' for the raster grid.
              
    Output: The resulting image resolution (not image size) should be an ``8'' by ``8'' grid image.
    The ``8'' rows and ``8'' columns should be represented in the spot layout display.

    Test Case Derivation: \an{see output?}

    How test will be performed: The user shall input the number of rows and columns.

    \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_subregion}}: Subregion / ROI\\}

    Control: Manual
                
    Initial State: \progname{} loaded and idle.
              
    Input: The magnification is set to ``2x'' and the raster grid is set to ``8 x 8''.
              
    Output: Both the spot layout and the resulting subregion displays should have ``8 x 8'' resolution (rows and columns). 
    The resulting image preview should have ``16 x 16'' resolution (rows and columns).

    Test Case Derivation: \an{see output?}

    How test will be performed: The user scroll/zoom in the Subregion (ROI) View display until a magnification of 2x is reached.

\end{enumerate}


\subsubsection{Image Quality Metric} \label{subsec_img_metric}

To satisfy R7, the user should be able to see a larger number for a resulting image that is closer to ground truth 
and a smaller number otherwise.

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_manualMetricHigh}}: High metric value (approximate to exact-sampling)\\}

    Control: Manual
              
    Initial State: \progname{} loaded and idle.
              
    Input: The Spot size is set to ``120\%'' with a perfectly circular spot shape.
              
    Output: The score should be greater than ``0.8650''.

    Test Case Derivation: The resulting image should look somewhat sharp as shown in the SRS figures \citep{SRS}.
					
  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_manualMetricLow}}: Low metric value (under-sampling)\\}

  Control: Manual
              
  Initial State: \progname{} loaded and idle.
            
  Input: The Spot size is set to ``10\%'' with a perfectly circular spot shape.
            
  Output: The score should be less than ``0.8601''.

  Test Case Derivation: The resulting image should look somewhat overly ``sharp'' or pixelated as shown in the SRS figures \citep{SRS}.
  					
  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_manualMetricLow2}}: Low metric value (over-sampling)\\}

  Control: Manual
              
  Initial State: \progname{} loaded and idle.
            
  Input: The Spot size is set to ``500\%'' with a perfectly circular spot shape.
            
  Output: The score should be less than ``0.8601''.

  Test Case Derivation: The resulting image should look somewhat ``blurry'' or ``defocused'' as shown in the SRS figures \citep{SRS}.
  					
  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_manualMetricControl}}: Control metric value\\}

  Control: Manual
              
  Initial State: \progname{} loaded and idle.
            
  Input: Spot size of a 100\%, the same pixel size (image resolution) as the ground truth image, 
  and perfectly circular spot shape.
            
  Output: A number that is near the best or maximum quality metric value. The resulting image should nearly
  identical to the ground truth image. The score should be greater than ``0.9800''.

  Test Case Derivation: The resulting image should be identical or have unperceivable differences to the naked eye.

\end{enumerate}


\subsection{Tests for Nonfunctional Requirements} \label{sec_NFR_tests}

The following tests will check if the nonfunctional requirements, as defined in the SRS \citep{SRS}, are 
met. The emphasis is on the \textit{usability} (NFR2) of the software. In the case of \progname{}, 
simplicity is more important than the actual complexity or correctness.
To satisfy the \textit{accuracy} (NFR1) requirement, an image quality and clarity survey shall be
conducted to establish what is perceived by the user and the image metric of what 
a ``better'' image is. To satisfy the \textit{maintainability} (NFR3) requirement, the code should 
follow a consistent style and reasonably pass static code analysis as described below.
As for \textit{portability} (NFR4), the user should be run the software on their platform and 
environment of choice.

\subsubsection{Usability Testing}
\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_surveyUsability}}: Usability Survey\\}

  Type: Manual
            
  Condition: A group of intended users, the SRS \citep{SRS}, and 
  the survey questions on topics related to \textit{usability}.
            
  Goal: The information collected from the completed user surveys (see \ref{survey_usability})
    will be aggregated and analyzed for any patterns and changes
    that may be incorporated to improve the software.
            
  How test will be performed: The users will be given a series of questions to evaluate the 
    ease-of-use, whether the GUI is intuitive or confusing, etc.

\end{enumerate}

\subsubsection{Accuracy Testing}
\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_surveyMetric}}: Image Metric Survey\\}

  Type: Manual
            
  Condition: A group of intended users, the SRS \citep{SRS}, 
  a list of images of varying quality, and the survey questions on topics related 
  to the image quality and clarity.
            
  Goal: The information collected from the completed user surveys (see \ref{survey_metric})
    will be aggregated and analyzed for any patterns to
    change or adapt the image metric used according to the rankings.
            
  How test will be performed: The users will be given a series of questions to evaluate 
    a given list of images, rank them based on detail and information content, etc.
  
\end{enumerate}

\subsubsection{Maintainability Testing}
\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_linters}}: Static code analysis\\}

  Type: Manual
            
  Condition: The software code will be tested through the use \textbf{pylint} for any
    Python code used and \textbf{ESLint} for any JavaScript code used.
            
  Goal: The code linters should list little to no 
    warnings (less than 10 unique warnings, on the default strictness setting), and zero errors.
            
  How test will be performed: Execute the \textbf{pylint} tool on all written Python code and 
    the \textbf{ESLint} tool on all written JavaScript code.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_codeReview}}: Code Review\\}

  Type: Manual
            
  Condition: The software source code and the code review checklist (see \ref{checklist_codeReview})
            
  Goal: The code should respect the defined code review checklist.
            
  How test will be performed: The author with walkthrough the code manually and check for qualities
    such as consistent styling, loose variables, variable scoping, unreachable code, 
    sufficient commenting, etc.
  
\end{enumerate}

\subsubsection{Portability Testing}
\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_portability}}: Various Platforms and Environments\\}

  Type: Manual
            
  Condition: Given any of the supported platforms, the user should be able to set up and 
    run the software with relative ease.
            
  Goal: The software runs on the platform of choice with little to no setup including 
    installation, meaning less than 4 clicks and under 3 minutes set up time.
            
  How test will be performed: The user will download and run the software on their
    platform of choice.
  
\end{enumerate}

\section{Unit Test Description} \label{sec_unittest}

This section cannot not be filled or completed until the MIS (detailed design document) has been completed.
\an{I did do some image metrics testing that can at least be partially documented here...}

\subsection{Unit Testing Scope}

\an{Other than the image metrics, I am not sure what else could be unit tested as of now?}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module: Image Quality Metric}

\an{I will likely use the ``reverse'' / ``inverse'' value as the image metric: \\
$Metric = 1 - NRMSE$, where the ``NRMSE'' is RMSE normalized to a value between 0 and 1.}
		
\begin{enumerate}

\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric1}}: Lowest/Highest MSE, RMSE and NRMSE scores\\}

Type: Automatic
					
Initial State: \an{Not running?}
					
Input/Condition: The ground truth images and multiple resampled images of varying and known quality.
					
Output/Result: The higher quality image should return the lowest error (i.e. lowest MSE, RMSE, NRMSE) scores.
					
How test will be performed: Executing a suite of tests for the multiple images.
					
\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metricControl}}: Control testing \\}

Type: Automatic
					
Initial State: \an{Not running?}
					
Input: The ground truth image.
					
Output: an error score of zero \an{or the lowest error possible depending on the given metric}.
					
How test will be performed: Executing a suite of tests using the different metric to compare the 
ground truth image to itself.
					
\item{test-id\#\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
be automatic}
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\newpage
\clearpage

\section{Traceability Between Test Cases and \\ Requirements}
Traceability matrices are used to simplify the process of identifying what needs to be changed 
if a component is modified. An ``X'' is used to indicate links between items in the table. 
When a component is changed, the elements marked with an ``X'' might need to be updated as well.

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    & R1
    & R2
    & R3
    & R4
    & R5
    & R6
    & R7
    & NFR1
    & NFR2
    & NFR3
    & NFR4
  \\ \hline
  \tref{T_inputImage}           &X& & & & & & & & & & \\ \hline
  \tref{T_exportImage}          & & & & & &X& & & & & \\ \hline
  \tref{T_spotSize}             & &X& & &X& & & & & & \\ \hline
  \tref{T_spotSize10}           & &X& & &X& & & & & & \\ \hline
  \tref{T_spotSize500}          & &X& & &X& & & & & & \\ \hline
  \tref{T_spotRotation}         & &X& & &X& & & & & & \\ \hline
  \tref{T_rasterGrid}           & & &X& &X& & & & & & \\ \hline
  \tref{T_subregion}            & & & &X&X& & & & & & \\ \hline
  \tref{T_manualMetricHigh}     & & & & & & &X& & & & \\ \hline
  \tref{T_manualMetricLow}      & & & & & & &X& & & & \\ \hline
  \tref{T_manualMetricLow2}     & & & & & & &X& & & & \\ \hline
  \tref{T_manualMetricControl}  & & & & & & &X& & & & \\ \hline
  \tref{T_surveyUsability}      & & & & & & & & &X& & \\ \hline
  \tref{T_surveyMetric}         & & & & & & & &X& & & \\ \hline
  \tref{T_linters}              & & & & & & & & & &X& \\ \hline
  \tref{T_codeReview}           & & & & & & & & & &X& \\ \hline
  \tref{T_portability}          & & & & & & & & & & &X\\ \hline
  \utref{U_metric1}             & & & & & & &X& & & &X\\ \hline
  \utref{U_metricControl}       & & & & & & &X& & & &X\\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between the Tests and Requirements}
  \label{Table:A_trace}
\end{table}

\newpage
\clearpage

\bibliographystyle{plainnat}
\bibliography{../../refs/References,../../refs/cas741}

\newpage

\section{Appendix}

This section contains the \textit{usability} and image quality (metric \textit{accuracy}) 
user survey questions, as well as a code review checklist.

\subsection{Usability Survey Questions} \label{survey_usability}

\begin{itemize}
  \item{What web browser do you use and what version is it?}
  \item{What operating system do you use?}
  \item{What is your screen resolution?}
  \item{Are you using a mouse and keyboard or a touchscreen?}
  \item{How much time (in seconds or minutes) did it take you to find the function you were looking for?}
  \item{What was the most useful feature for you?}
  \item{On a scale from 1 to 10, how easy was it to set up and start the software?}
  \item{On a scale from 1 to 10, how easy was it to use the software?}
  \item{What was the least enjoyable feature to use?}
  \item{Was everything legible and clearly displayed? If not, what was not?}
  \item{Would you consider using the software as a learning or teaching tool?}
  \item{Do you have any extra comments or thoughts you'd like to share?}
\end{itemize}

\subsection{Image Quality Survey Questions} \label{survey_metric}
Given multiple images that are numbered and a ground truth image, 
the user shall answer the following questions?

\begin{itemize}
  \item{Which image looks the seems to be the clearest or is of the high quality in your opinion?}
  \item{Which image provide the most information or detail?}
  \item{Which image is the most similar to or the most representative of the ground truth image?}
  \item{Which image is the least pleasant in your opinion?}
  \item{Rank the images from the lowest quality (least clear) to the highest quality (most clear) in your opinion?}
  \item{Rank the images from least to most informative (or has most of the details of the ground truth image)?}
  \item{The user is shown the ground truth image and the reproduced control image 
  (generated using the same image resolution and exact-sampling). 
  Do these images look identical to you? If not, on a scale of 1 to 10, how similar are these images?}
\end{itemize}

\subsection{Code Review Checklist} \label{checklist_codeReview}

\begin{itemize}
  \item{Does the code follow a consistent style?}
  \item{Are there any variables that should not be global?}
  \item{Are function names not too long (i.e. less than 40 characters)?}
  \item{Are most function well named and clear in what they do?}
  \item{Are there any long lines of that can be split into multiple lines?}
  \item{Are global variables and functions documented?}
  \item{Is each function not too long and sufficiently broken down to accomplish a single task?}
  \item{Are comments plentiful, but not noisy?}
  \item{Is the code sorted into separate files where reasonable?}
  \item{Is there any duplicate that code be avoided?}
  \item{Is there any use of jargon, domain-specific, or unclear terms?}
  \item{Is all of the code reachable?}
  \item{Could programmer other than the author read the code and briefly understand it?}
  \item{Is there any leftover commented code that is not useful?}
\end{itemize}

\end{document}