\documentclass[12pt, titlepage]{article}

\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{float}
\usepackage{siunitx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
% \usepackage[round]{natbib}
\usepackage[numbers,square]{natbib}

\usepackage{xr}
\externaldocument{../SRS/SRS}
\externaldocument{../Design/MG/MG}

\newcommand{\rref}[1]{R\ref{#1}}
\newcommand{\nfrref}[1]{NFR\ref{#1}}
\newcommand{\mref}[1]{M\ref{#1}}

\newcommand{\ttref}[1]{Table \ref{#1}}


\newcounter{testnum} %test Number
\newcommand{\dthetestnum}{T\thetestnum}
\newcommand{\tref}[1]{T\ref{#1}}

\newcounter{unittestnum} %unittest Number
\newcommand{\dtheunittestnum}{U\theunittestnum}
\newcommand{\utref}[1]{U\ref{#1}}

\newcounter{libnum} %lib Number
\newcommand{\dthelibnum}{L\thelibnum}
\newcommand{\lref}[1]{L\ref{#1}}
\newcommand{\lrefp}[1]{(\lref{#1})}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{2.2cm}p{1.8cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2023/02/17 & 1.0 & First version \\
2023/02/20 & 1.0.1 & fix minor issues in T\ref{T_exportImage}, T\ref{T_spotSize}, T\ref{T_linters}, 
  and T\ref{T_portability}\\
  & 1.0.1 & Minor issues in general information\\
  & 1.0.2 & Fix grammar in sections \ref{sec_srs_vplan}, 
  \ref{sec_design_vplan}, \ref{sec_impl_vplan}, and \ref{sec_autotest_tools}. \\
  & 1.0.3 & Fix formatting and grammar in sections \ref{sec_systest_desc}, 
  \ref{subsec_img_io}, \ref{subsec_spot}, and \ref{subsec_img_metric}. \\
  & 1.0.4 & Fix grammar in Section \ref{sec_NFR_tests}. \\
  & 1.0.5 & Tweaks to Section \ref{sec_objectives}. \\
  & 1.0.6 & Added missing references to MG and MIS. \\
2023/02/22 & 1.0.7 & Provide specific values for most of the tests. \\
  & 1.0.8 & Review the design and VnV verification plans. \\
2023/02/27 & 1.0.9 & Improve wording for Section \ref{sec_impl_vplan}. \\
  & 1.0.10 & Tweak tests in sections \ref{subsec_spot} and \ref{subsec_img_metric}. \\
2023/03/23 & 1.0.11 & Improve clarity for how some tests will be performed.\\
2023/03/24 & 1.1.0 & Incorporate feedback from Dr. Smith's review.\\
  & 1.1.1 & Added a code linting ruleset.\\
  & 1.1.2 & Added symbolic constants.\\
2023/03/25 & 1.2.0 & Fill in parts of the unit testing (section \ref{sec_unittest}). \\
  & 1.2.1 & Added details in the Appendix (\ref{sec_Appendix}). \\
2023/03/26 & 1.2.2 & Fill in expected outputs and justifications for
  the unit tests in section \ref{sec_func_req_unittest} and \ref{sec_nfr_unittest}. \\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  GUI & Graphical User Interface\\
  MG & Module Guide \\
  MIS & Module Interface Specification \\
  MSE & Mean Squared Error\\
  MS-SSIM & Multi-scale Structural Similarity\\
  NFR & Nonfunctional Requirement\\
  NRMSE & Normalized Root Mean Square Error\\
  PSNR & Peak Signal-to-Noise Ratio\\
  R & Requirement\\
  RMSE & Root Mean Square Error\\
  ROI & Region of Interest\\
  SCC & Spatial Correlation Coefficient\\
  SRS & Software Requirements Specification\\
  SSIM & Structural Similarity\\
  T & Test\\
  U & Unit test\\
  UI & User Interface\\
  UIQ & Universal Image Quality\\
  VIF & Visual Information Fidelity \\
  VnV & Verification and Validation\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}

\section{General Information}

This document lays out the Verification and Validation (VnV) plan of \progname{}
as described by the Software Requirements Specification (SRS) 
\citep{SRS}. Testing of the software and its components will be conducted to build confidence in 
its usability, accuracy, and, ultimately, whether it meets the requirements from the SRS.

\subsection{Summary}

\progname{} software aims to be a demonstration and learning tool of how a scanning electron 
microscope (SEM) formulates an image. The idea is to help visualize the influence trends of imaging 
parameters (as defined in the SRS) on image quality (or clarity) and resolution.


\subsection{Objectives} \label{sec_objectives}

The objective is to produce a tool that is easy to use (\textit{usability}). It should feel intuitive 
to the user and should provide easy-to-understand information. This is so the user may 
identify any trends in how the final image is affected with relative ease and confidence. As a 
secondary goal, the \textit{accuracy} in the trends are important. Due to the subjective and 
qualitative nature of evaluating image quality, the metric used should be straight-forward and 
provide some objectivity (for quantitative comparison) and assurance to the users.
As for an objective that will be left out, due to resource limitations, is the study and
improvement of an all-round robust (quantitative and consistent) image metric. While this
is a highly desired goal, this is an incredibly difficult task and is 
currently deemed to be outside the scope of this project.

\subsection{Relevant Documentation}

There are multiple design documents that provide the important and intimate details to understand 
some of the concepts that are being tested. These documents are the following:

\begin{itemize}
  \item Problem Statement \citep{Prob_Statement}: States the problem we are trying to solve
    and introduces the topics that will need be verified.
  \item SRS \citep{SRS}: States the requirements, assumptions, models, and important 
  terminology that are used in the VnV plan.
  \item VnV Report \citep{VnV_report}: States what has been achieved from the VnV plan,
    as well as provide results or evidence to help build confidence for correctness.
  \item MG \citep{MG}: States the responsibilities and secrets for each module that will
    be evaluated by the VnV plan.
  \item MIS \citep{MIS}: Specifies the functional design and states what is needed for each
    module that will be evaluated by the VnV plan.
\end{itemize}

\section{Plan}

In this section, multiple plans are described to test and inspect the software with an emphasis 
on \textit{usability}. Multiple approaches and perspectives will be employed by the VnV team (\ref{sec_vnv_team})
to help build confidence in the requirements, to avoid any missed important details, 
and to deliver on the qualities as mentioned in the objective (\ref{sec_objectives}).

\newpage

\subsection{Verification and Validation Team} \label{sec_vnv_team}

The members of the VnV team as well their individual roles are listed in the following table:

\begin{table}[h!]
  \centering
  \begin{tabular}{|r|l|}
    \rowcolor[gray]{0.9}
    \hline
    \textbf{Role} & \textbf{Name} \\ \hline
    Project Supervisor & Dr.\ Spencer Smith  \\ \hline
    Author             & \authname           \\ \hline
    Domain Expert      & Karen Wang          \\ \hline
    SRS Reviewer       & Jason Balaci        \\ \hline
    VnV Plan Reviewer  & Sam Crawford        \\ \hline
    MG + MIS Reviewer  & Lesley Wheat        \\ \hline
    Expert Consultant  & Dr.~Nabil Bassim    \\ \hline
    Expert Consultant  & Michael W.~Phaneuf  \\ \hline
  \end{tabular}
  \caption{Table of the VnV Team Members}
  \label{table_vnv_team}
\end{table}

\noindent \textit{\small{Note: Dr.~Bassim is my PhD Supervisor, and Mr.~Phaneuf
is my co-supervisor (employer/industry), for an industrial PhD.
Much of this project was originally conceptualized with 
the guidance and help of Mr.~Phaneuf.
This is acknowledged here.}}


\subsection{SRS Verification Plan} \label{sec_srs_vplan}

The SRS will be reviewed by the project supervisor, the SRS reviewer and the author. Some input
may be given by the expert consultants if time permits. Most of the feedback has been provided 
as issues on GitHub, as annotated documents, or by verbal exchange. 
Throughout the development of the project, the author is expected 
to make changes needed to the resolve the issues. The reviewers may refer to the SRS checklist \citep{SRS_checklist}.
The key objective is to verify that the software requirements and the documentation are sound 
and coherent to the intended audience as defined in the SRS.


\subsection{Design Verification Plan} \label{sec_design_vplan}

Much of the conceptualization was done after having multiple discussions with the expert 
consultants and having done literature review for preexisting tools and documentation 
relevant to the project. Decisions were made with a focus on the usability, meaning little to 
no setup required. The design and implementation is documented in the 
MG\citep{MG}/MIS\citep{MIS} which will be verified with the MG \cite{MG_checklist}
and MIS \cite{MIS_checklist} checklists.
The VnV team, as well as any welcomed volunteers, will provide
their input (through GitHub issues). Eventually, if the project were to be published
in a journal (to be determined) where the software \progname~and its
accompanying documentation will likely be rigorously reviewed; the expert
consultants will review and evaluate the software to
establish whether it is fit for public use.


\subsection{Verification and Validation Plan Verification Plan}

The goal is to uncover any mistakes and reveal any risks (such as misconceptions or coverage gaps) 
through the supervision and 
review of the VnV team members. Once most of the work has been done, the work and
accompanying documentation shall undergo a vetting process: the VnV team
will check whether the documented testing plans and verification process have been 
accomplished and the requirements fulfilled. The author will then review the documents once more
against the VnV checklist \citep{VnV_checklist}, before a final review by the rest of the VnV team.

\subsection{Implementation Verification Plan}  \label{sec_impl_vplan}

Much of the software will be tested manually by users. This will include checking for
any inconsistencies, bugs in the graphical user interface (GUI), and unexpected
artifacts in images produced.
The image metrics will be tested using unit tests (Section \ref{sec_unittest}). For the
all the code implemented, linters will be used as mentioned in Section \ref{sec_autotest_tools}.
As a control for the image metrics, they will be calculated using the ground truth image as both the 
input and the reference image (effectively comparing it to itself) 
to rule out any baseline or unexpected factors in the implementations themselves.


\subsection{Automated Testing and Verification Tools} \label{sec_autotest_tools}

The image quality metric shall be unit tested using \href{https://pytest.org}{pytest} for 
automated testing of any algorithms implemented in Python and \href{https://qunitjs.com}{QUnit} 
shall be used for those implemented in JavaScript. The unit tests are listed in 
Section \ref{sec_unittest}.
As for linters, \href{https://flake8.pycqa.org}{flake8} shall be used for Python code 
and \href{https://eslint.org}{ESLint} for JavaScript code.
The \href{https://github.com/andrewekhalel/sewar}{sewar} python package will be 
used as a reference implementation in Python for the image quality metrics.


\subsection{Software Validation Plan}
A \textit{usability} survey will be conducted to evaluate the user experience and whether 
the GUI is intuitive enough to the intended users as described in the SRS \citep{SRS}.
An \textit{accuracy} survey will be conducted to assess the user-perceived image quality. 
The trends identified in the surveys results will be compared to the calculated image metrics.
As a control for the images produced, a manual test (see \tref{T_reproduceGT})
will be conducted to verify if 
an image identical (or with unperceivable difference) to ground truth image can be reproduced.
The compared images shall be in the \textit{accuracy} survey for confirmation. This can be compared
as well using the image metrics. 


\section{System Test Description} \label{sec_systest_desc}

In this section, the system tests that will be conducted are described in detail. These tests
will be used to verify the fulfillment of the requirements as listed in the SRS \citep{SRS}.
Most, if not all, of the tests listed here will be manually performed. Automatic
tests will be unit tests as described in Section \ref{sec_unittest}.

\subsection{Tests for Functional Requirements}

The tests present will verify whether the functional requirements 
(as listed in the SRS \cite{SRS}) are met and validate that the outputs
are as expected.

\subsubsection{Image Import and Export} \label{subsec_img_io}

To satisfy R1 from the SRS \citep{SRS}, any input image of the following formats listed below shall be 
accepted provided they follow the input data constraints.
Some preloaded example images shall be included and be available to the user for use as well.
As well as for R6, the software should allow for export of the resulting image.

\begin{itemize}
  \item{PNG}
  \item{JPG}
  \item{BMP\\}
\end{itemize}

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_inputImage}}: Test import for PNG/JPG/BMP format\\}
            
  Control: Manual
            
  Initial State: \progname{} loaded and idle.
            
  Input: The default preloaded image or any non-corrupt (valid) PNG, JPG, or BMP image file of the testers choice.
            
  Output: The image should be visible and be displayed as expected as the ground truth image.
    This will be judged and verified by manual visual inspection.
            
  Test Case Derivation: These are some of the most common image file formats and should be compatible with the software.
            
  How test will be performed: The user will click the ``Load image'' button and select an image to import.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_exportImage}}: Test PNG Image Export\\}
              
  Control: Manual
            
  Initial State: \progname{} loaded and idle.
            
  Input: User clicks the \textit{Export} button.
            
  % Output: \wss{The expected result for the given inputs}
  Output: The output image file should look the same as what is displayed in the ``Resulting Image'' display.
    This will be judged and verified by manual visual inspection.

  % Test Case Derivation: \wss{Justify the expected value given in the Output field}
  Test Case Derivation: Not applicable.

  How test will be performed: The user will click the ``Export image'' button and choose where to save the image file.

\end{enumerate}


\subsubsection{Spot Profile and Imaging Parameters} \label{subsec_spot}

To satisfy R2 and R5 from the SRS \citep{SRS}, the software should accept input from the user
to change the width, height, and rotation which, define an ellipsoid shape. 
This shape is then used as the spot to sample the ground truth image.
To satisfy R3, the software should accept user input for a real positive number for the pixel size.
To satisfy R4, the user should be able to specify a subregion (or ROI) for processing.

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_spotSize}}: Spot Width and Height - Exact-sampling\\}

    Control: Manual
              
    Initial State: \progname{} loaded and idle.
              
    Input: Both width and height are set to ``100\%''.
              
    Output: The spot layout should reflect these changes and display an updated arrangement with the given shape.
      The resulting image should exhibit the expected \textbf{exact-sampling} case as documented by the SRS figures \citep{SRS}.
      This will be judged and verified by manual visual inspection.

    Test Case Derivation: Not applicable.
              
    How test will be performed: Manually, either by using the mouse scroll wheel in
      the spot content display, or by double-clicking on the width display and entering '100'
      in prompt that appears.

    \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_spotSize10}}: Spot Width and Height - Under-sampling\\}

    Control: Manual
              
    Initial State: \progname{} loaded and idle.
              
    Input: Both width and height are set to ``10\%''.
              
    Output: The spot layout should reflect these changes and display an updated arrangement with the given shape.
      The resulting image should exhibit the expected \textbf{under-sampling} case as documented by the SRS figures \citep{SRS}.
      This will be judged and verified by manual visual inspection.

    Test Case Derivation: Not applicable.
              
    How test will be performed: Manually, either by using the mouse scroll wheel in
      the spot content display, or by double-clicking on the width display and entering '10'
      in prompt that appears.

    \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_spotSize500}}: Spot Width and Height - Over-sampling\\}

    Control: Manual
              
    Initial State: \progname{} loaded and idle.
              
    Input: Both width and height are set to ``500\%''.
              
    Output: The spot layout should reflect these changes and display an updated arrangement with the given shape.
      The resulting image should exhibit the expected \textbf{over-sampling} case as documented by the SRS figures \citep{SRS}.
      This will be judged and verified by manual visual inspection.

    Test Case Derivation: Not applicable.
              
    How test will be performed: Manually, either by using the mouse scroll wheel in
      the spot content display, or by double-clicking on the width display and entering '500'
      in prompt that appears.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_spotRotation}}: Spot Rotation - Astigmatism\\}

    Control: Manual
                
    Initial State: \progname{} loaded and idle.
              
    Input: The angle is set to ``45 degrees'', the width is set ``60\%'', and the height is set to ``500\%''.
              
    Output: The spot layout should reflect the updated arrangement with the rotated spots.
      The resulting image should exhibit the expected \textbf{astigmatism} effects as documented in the SRS figures \citep{SRS}.
      This will be judged and verified by manual visual inspection.

    Test Case Derivation: Not applicable.

    How test will be performed: How test will be performed: Manually, either by using the mouse scroll wheel in
    the spot content display, or by double-clicking on the width display and entering '100'
    in prompt that appears. And, by dragging the rotation node in the spot profile display
    until an angle of 45 is reached.

    \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_rasterGrid}}: Raster Grid / Pixel Size\\}

    Control: Manual
                
    Initial State: \progname{} loaded and idle.
              
    Input: The rows and columns are set to ``8'' for the raster grid.
              
    Output: The resulting image resolution (not image size) should be an ``8'' by ``8'' grid image.
      The ``8'' rows and ``8'' columns should be represented in the spot layout display.
      This will be judged and verified by manual visual inspection.

    Test Case Derivation: Not applicable.

    How test will be performed: The user shall input the number of rows and columns.

    \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_subregion}}: Subregion / ROI\\}

    Control: Manual
                
    Initial State: \progname{} loaded and idle.
              
    Input: The magnification is set to ``2x'' and the raster grid is set to ``8 x 8''.
              
    Output: Both the spot layout and the resulting subregion displays should have ``8 x 8'' resolution (rows and columns). 
      The resulting image preview should have ``16 x 16'' resolution (rows and columns).
      This will be judged and verified by manual visual inspection.

    Test Case Derivation: Not applicable.

    How test will be performed: The user scroll/zoom in the Subregion (ROI) View display until a magnification of 2x is reached.

    \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_reproduceGT}}: Ground Truth Reproduction\\}

    Control: Manual
                
    Initial State: \progname{} loaded and idle.
              
    Input: An image that is 400x400 pixels is loaded.
      The rows and columns are set to ``200'' for the raster grid.
      The magnification is set to ``2x''.
      Both the spot width and height are set to ``100\%''.
              
    Output: The resulting image shall look nearly identical to input image.
      This will be judged and verified by manual visual inspection.

    Test Case Derivation: Not applicable.

    How test will be performed: The user shall input the number of rows and columns,
      set the magnification by using the mouse scroll wheel in the Subregion/ROI display,
      and set the spot size either by using the mouse scroll wheel in
      the spot content display, or by double-clicking on the width display and entering '100'
      in prompt that appears.

\end{enumerate}


\subsubsection{Image Quality Metric} \label{subsec_img_metric}

To satisfy R7, the user should be able to see a larger number for a resulting image that is closer to ground truth 
and a smaller number otherwise. The ground truth image used for these tests shall be the default
preloaded image or any valid (as per the SRS \cite{SRS}) image of the tester's choice that is
400 by 400 pixels. Most of these test are conducted manually and judge visually by the tester.
As for automated testing, there are control tests for the image metric in section \ref{sec_unittest}.

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_manualMetricHigh}}: High metric value (approximate to exact-sampling)\\}

    Control: Manual
              
    Initial State: \progname{} loaded and idle.
              
    Input: The Spot size is set to ``120\%'' with a perfectly circular spot shape.
      The rows and columns are set to ``16''.
      The magnification is set to ``2x''.
              
    Output: The score should be greater than ``0.8501''.

    Test Case Derivation: The resulting image should look somewhat sharp as shown in the SRS figures \citep{SRS}.
      This will be judged and verified by manual visual inspection.
      Preliminary tests were conducted to obtain the score value shown above.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_manualMetricLow}}: Low metric value (under-sampling)\\}

  Control: Manual
              
  Initial State: \progname{} loaded and idle.
            
  Input: The Spot size is set to ``10\%'' with a perfectly circular spot shape.
    The rows and columns are set to ``16''.
    The magnification is set to ``2x''.
            
  Output: The score should be less than ``0.8501''.

  Test Case Derivation: The resulting image should look somewhat overly ``sharp'' or
    pixelated as shown in the SRS figures \citep{SRS}.
    This will be judged and verified by manual visual inspection.
    Preliminary tests were conducted to obtain the score value shown above.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_manualMetricLow2}}: Low metric value (over-sampling)\\}

  Control: Manual
              
  Initial State: \progname{} loaded and idle.
            
  Input: The Spot size is set to ``500\%'' with a perfectly circular spot shape.
    The rows and columns are set to ``16''.
    The magnification is set to ``2x''.
            
  Output: The score should be less than ``0.8501''.

  Test Case Derivation: The resulting image should look somewhat ``blurry'' or ``defocused''
    as shown in the SRS figures \citep{SRS}.
    This will be judged and verified by manual visual inspection.
    Preliminary tests were conducted to obtain the score value shown above.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_manualMetricControl}}: Control metric value\\}

  Control: Manual
              
  Initial State: \progname{} loaded and idle.
            
  Input: Spot size of a 100\%, the same pixel size (image resolution) as the ground truth image, 
    and perfectly circular spot shape.
    The rows and columns are set to ``200''.
    The magnification is set to ``2x''.
            
  Output: A number that is near the best or maximum quality metric value
    (as specified in the SRS \cite{SRS}). The resulting image should nearly
    identical to the ground truth image. This will be judged and verified by manual visual inspection.
    However, The score should be ``1.00'' (meaning a perfect score), but to allow
    for a small error margin, it should be greater than ``0.9800''.

  Test Case Derivation: The resulting image should be identical or have unperceivable differences to the naked eye.
    This will be judged and verified by manual visual inspection.
    Preliminary tests were conducted to obtain the score value shown above.

\end{enumerate}


\subsection{Tests for Nonfunctional Requirements} \label{sec_NFR_tests}

The following tests will check if the nonfunctional requirements, as defined in the SRS \citep{SRS}, are 
met. The emphasis is on the \textit{usability} (NFR2) of the software. In the case of \progname{}, 
simplicity is more important than the actual complexity or correctness.
To satisfy the \textit{accuracy} (NFR1) requirement, an image quality and clarity survey shall be
conducted to establish what is perceived by the user and the image metric of what 
a ``better'' image is. To satisfy the \textit{maintainability} (NFR3) requirement, the code should 
follow a consistent style and reasonably pass static code analysis as described below.
As for \textit{portability} (NFR4), the user should be run the software on their platform and 
environment of choice.

\subsubsection{Usability Testing}
\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_surveyUsability}}: Usability Survey\\}

  Type: Manual
            
  Condition: A group of intended users, the SRS \citep{SRS}, and 
  the survey questions on topics related to \textit{usability}.
            
  Result: The completed user surveys (see \ref{survey_usability}).

  Note: The information collected will be aggregated and analyzed for any patterns and changes
    that may be incorporated to improve the software.
            
  How test will be performed: The users will be given a series of questions to evaluate the 
    ease-of-use, whether the GUI is intuitive or confusing, etc.

\end{enumerate}

\subsubsection{Accuracy Testing}
\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_surveyMetric}}: Image Metric Survey\\}

  Type: Manual
            
  Condition: A group of intended users, the SRS \citep{SRS}, 
  a list of images of varying quality, and the survey questions on topics related 
  to the image quality and clarity.
            
  Result: The completed user surveys (see \ref{survey_metric}).

  Note: The information collected from the surveys
    will be aggregated and analyzed for any patterns to
    change or adapt the image metric used according to the rankings.
            
  How test will be performed: The users will be given a series of questions to evaluate 
    a given list of images, rank them based on detail and information content, etc.
  
\end{enumerate}

\subsubsection{Maintainability Testing}
\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_linters}}: Static code analysis\\}

  Type: Manual
            
  Condition: The software code will be tested through the use \textbf{pylint} for any
    Python code used and \textbf{ESLint} for any JavaScript code used.
            
  Result: The code linters should list zero 
    warnings (using a specified ruleset, see \ref{codelint_ruleset}), and zero errors.
            
  How test will be performed: Execute the \textbf{pylint} tool on all written Python code and 
    the \textbf{ESLint} tool on all written JavaScript code.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_codeReview}}: Code Review\\}

  Type: Manual
            
  Condition: The software source code and the code review checklist (see \ref{checklist_codeReview})
            
  Result: The code should respect the defined code review checklist.
            
  How test will be performed: The author with walkthrough the code manually and check for qualities
    such as consistent styling, loose variables, variable scoping, unreachable code, 
    sufficient commenting, etc.
  
\end{enumerate}

\subsubsection{Portability Testing}
\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_portability}}: Various Platforms and Environments\\}

  Type: Manual
            
  Condition: Given any of the supported platforms, the user should be able to set up and 
    run the software with relative ease.
            
  Result: The software runs on the platform of choice with little to no setup including 
    installation, meaning less than \code{SETUP\_MAX\_CLICKS}
    clicks and under \code{SETUP\_MAX\_TIME}
    setup time (see symbolic constants in section \ref{secsymbolic_constants}).
            
  How test will be performed: The user will download and run the software on their
    platform of choice.
  
\end{enumerate}



\section{Unit Test Description} \label{sec_unittest}

This section focuses on automated testing for individual modules as defined by the
MG \cite{MG} and MIS \cite{MIS}. 

\subsection{Unit Testing Scope} \label{sec_unittest_scope}

Currently, there's only automated testing of the image metrics calculation (\mref{M_metric}).
Much of the testing planned is manually due to the complex and subjective nature of 
image quality. 
Most of the drawing functionality is handled by the Konva javascript library \cite{konva_2021}
and thus will not be verified.
Although it is possible to test GUIs and visual fidelity to some degree, this
is not planned here to due to limited resources (mainly time).

A suite of images (\ref{Tbl_TestImages}) is used for testing is provided (with the source code).

\newcounter{tImg}
\newcommand{\timg}{\stepcounter{tImg}\arabic{tImg}}

\begin{table}[H]
  \centering
  \begin{tabular}{L{.75cm} L{4.5cm} L{1cm} L{7cm}}
  \toprule
    \textbf{No.} & \textbf{File Name} & \textbf{Code} & \textbf{Description} \\
  \midrule
  \multicolumn{4}{c}{Ground Truth Images} \\
  \hline
  \timg & \code{original\_500-crop.png} & \code{og} & a virtual image representing metallic grains. \\
  \timg & \code{og-fant.png} & \code{fant} & the original image resampled using
      Fant's algorithm \cite{fant_1986} as 50x50 pixel image 
      and scaled back up using nearest neighbor. \\
  \hline
  \multicolumn{4}{c}{Control Images} \\
  \hline
    \timg & \code{black.png} & \code{blak} & all black (\code{0x00} or \code{0}) pixels. \\
    \timg & \code{white.png} & \code{wite} & all white (\code{0xFF} or \code{255}) pixels. \\
    \timg & \code{half.png} & \code{half} & half white, half black pixels. \\
    \timg & \code{gray.png} & \code{gray} & all pixels are gray (\code{0x80} or \code{128},
      half of the ``colour range'' of the 8 bit depth). \\
    \timg & \code{og-minor\_defects.png} & \code{mDef} & the original image manually
      edited to have a few pixels set to white to simulate minor defects. \\
  \hline
  \multicolumn{4}{c}{Test Images} \\
  \hline
    \timg & \code{c10-010-010.png} & \code{c010} & 10x pixel size, sampled with a spot of 10\% by 10\% \\
    \timg & \code{c10-060-060.png} & \code{c060} & 10x pixel size, sampled with a spot of 60\% by 60\% \\
    \timg & \code{c10-100-100.png} & \code{c100} & 10x pixel size, sampled with a spot of 100\% by 100\% \\
    \timg & \code{c10-120-120.png} & \code{c120} & 10x pixel size, sampled with a spot of 120\% by 120\% \\
    \timg & \code{c10-130-130.png} & \code{c130} & 10x pixel size, sampled with a spot of 130\% by 130\% \\
    \timg & \code{c10-500-500.png} & \code{c500} & 10x pixel size, sampled with a spot of 500\% by 500\% \\
    \timg & \code{c10-060-500.png} & \code{c6a5} & 10x pixel size, sampled with a spot of 60 and 500\% at \ang{45}\\
  \bottomrule
  \end{tabular}
  \caption{The suite of test images used (all the same size, 500 x 500 px).}
  \label{Tbl_TestImages}
\end{table}


\newpage

\subsection{Tests for Functional Requirements} \label{sec_func_req_unittest}

See unit testing scope (\ref{sec_unittest_scope}) for more information.
\progname{}'s image metric is the only module that will be unit tested to
satisfy \rref{R_imageMetric}. The metric is tested against the control images
(\code{black.png}, \code{white.png}, and \code{gray.png})
from the suite of test images, to assess whether the metric
is working as anticipated.
A table of the results shall be presented in the VnV report \cite{VnV_report}.

\subsubsection{Module: Image Metrics Calculation (\mref{M_metric})}

\begin{itemize}

\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric0A}}: control - ground truth vs. ground truth}

Type: Automatic

Input: \code{original\_500-crop.png} and \code{original\_500-crop.png} (see \ttref{Tbl_TestImages})

Output: \code{1.00} $\pm$ ERR\_MARGIN

Test Case Derivation: Should be the highest score/match possible
  since they are the same image / identical.

How test will be performed: Running the javascript unit tests.


\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric0B}}: control - ground truth vs. minor defects}

Type: Automatic

Input: \code{original\_500-crop.png} and \code{og-minor\_defects.png} (see \ttref{Tbl_TestImages})

Output: \code{0.99} $\pm$ ERR\_MARGIN

Test Case Derivation: Should be very close to the highest score/match possible
  since they are nearly identical.

How test will be performed: Running the javascript unit tests.


\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric1}}: control - all black vs. all white}

Type: Automatic

Input: \code{black.png} and \code{white.png} (see \ttref{Tbl_TestImages})

Output: \code{0.00} $\pm$ ERR\_MARGIN

Test Case Derivation: Should be the lowest score/match possible
  since they are complete opposites.

How test will be performed: Running the javascript unit tests.


\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric2}}: control - all black vs. half black and white}

Type: Automatic

Input: \code{black.png} and \code{half.png} (see \ttref{Tbl_TestImages})

Output: \code{0.29} $\pm$ ERR\_MARGIN

Test Case Derivation: Should be $1 - \frac{1}{255} \sqrt{\text{MSE}}$ (following the formula for iNRMSE)
as defined in the SRS \cite{SRS}), where the Mean Square Error (or difference)
should be $\frac{1}{2} \cdot (255-0)^2 = 32512.5$ (i.e. half ``different''
meaning half the maximum).

How test will be performed: Running the javascript unit tests.


\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric3}}: control - all white vs. half black and white}

Type: Automatic

Input: \code{white.png} and \code{half.png} (see \ttref{Tbl_TestImages})

Output: \code{0.29} $\pm$ ERR\_MARGIN

Test Case Derivation: Same justification as \utref{U_metric2}.

How test will be performed: Running the javascript unit tests.


\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric4}}: control - all black vs. all gray}

Type: Automatic

Input: \code{black.png} and \code{gray.png} (see \ttref{Tbl_TestImages})

Output: \code{0.50} $\pm$ ERR\_MARGIN

Test Case Derivation: Should be $1 - \frac{1}{255} \sqrt{\text{MSE}}$ (following the formula for iNRMSE)
as defined in the SRS \cite{SRS}), where the Mean Square Error (or difference)
should be $(0-128)^2 = 16384$.

How test will be performed: Running the javascript unit tests.


\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric5}}: control - all white vs. all gray}

Type: Automatic

Input: \code{white.png} and \code{gray.png} (see \ttref{Tbl_TestImages})

Output: \code{0.50} $\pm$ ERR\_MARGIN

Test Case Derivation: Same justification as \utref{U_metric4}, 
with the slight difference that the MSE is $(255-128)^2 = 16129$.

How test will be performed: Running the javascript unit tests.


\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric6}}: control - half black and white vs. all gray}

Type: Automatic

Input: \code{half.png} and \code{gray.png} (see \ttref{Tbl_TestImages})

Output: \code{0.50} $\pm$ ERR\_MARGIN

Test Case Derivation: One half of the pixels follow the justification of \utref{U_metric4},
and the other half follow the justification of \utref{U_metric5}.

How test will be performed: Running the javascript unit tests.

    
\end{itemize}


\newpage

\subsection{Tests for Nonfunctional Requirements} \label{sec_nfr_unittest}

See unit testing scope (\ref{sec_unittest_scope}) for more information.
The image metrics is the only module that will be unit tested to
satisfy \nfrref{NFR_Accuracy}.

The image metric needs to be robust and accurate enough for our usecase.
The \href{https://pypi.org/project/sewar}{sewar}
(\textbf{L\refstepcounter{libnum}\thelibnum \label{lib_sewar}})
python package (which implements MSE, RMSE, PSNR, UIQ, SSIM, MS-SSIM, SCC, VIF, and more)
and following javascript libraries are used for ``oracle'' comparisons
to \progname{}'s metric
(\textbf{L\refstepcounter{libnum}\thelibnum \label{lib_prog}}):
\begin{itemize}
  \item \href{https://github.com/obartra/ssim}{ssim.web-3.5.0.js} (\textbf{L\refstepcounter{libnum}\thelibnum \label{lib_ssim}})
    -- implements SSIM
  \item \href{https://github.com/darosh/image-mse-js}{image-mse.js} (\textbf{L\refstepcounter{libnum}\thelibnum \label{lib_mse}})
    -- implements MSE, PNSR
  \item \href{https://github.com/darosh/image-ssim-js}{image-ssim.js} (\textbf{L\refstepcounter{libnum}\thelibnum \label{lib_ssim1}})
    -- implements SSIM
  \item \href{https://github.com/darosh/image-ms-ssim-js}{image-ms-ssim.js} (\textbf{L\refstepcounter{libnum}\thelibnum \label{lib_msssim}})
    -- implements MS-SSIM and SSIM
\end{itemize}

Various libraries with different algorithms/metrics are used here to see how each
performs in terms of accuracy using the suite of test images.
The expected output values presented here are qualitative as the specific values
of each metric for following tests are unknown and may vary widely
based on the given images (except for the control images).

A table of the results shall be presented in the VnV report \cite{VnV_report}.



\subsubsection{Module: Image Metrics Calculation (\mref{M_metric})}

\begin{itemize}

\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric7}}:
Image quality ranking}

Type: Automatic

Input/Condition: The ground truth images (\code{og} and \code{fant}) compared to each of
  the resampled images of varying quality (the ``test images''
  listed in Table \ref{Tbl_TestImages}).


Output/Result: The expected order (from low to high) of the metric scores is
$
\code{c500} \le \code{c6a5} \le \code{c010} \le \code{c060} \le
\code{c100} \le \code{c130} \le \code{c120}
$.
For the following reasons:
\begin{itemize}
  \item \code{c500} grossly oversamples in all directions
  \item \code{c6a5} is the same, but moderately/reasonably undersamples in one direction
  \item \code{c010} grossly undersamples, but conversely
    the error is localized to each raster cell and not spread out
  \item \code{c060} similar, but undersamples to a lesser degree
  \item \code{c100} should score reasonably well
  \item \code{c130} should score slightly better as there is more coverage, but may spread 
    the error more
  \item \code{c120} should score the highest as reasonable compromise between area coverage
    and error spread
\end{itemize}


Caveats:
\begin{enumerate}
  \item These scores are dependent on the raster grid (number of cells and size of
    the cell or ``pixels''). For example, if we only had one large pixel/cell, then
    the whole image is averaged into one value/signal. If we had a 100 million cells/pixels
    in both width and height on an image that was only 500 x 500 pixels, then the scores
    would all be the same (or close to) by averaging/sampling all the same values and
    relative areas. That is, unless one test would sample with a 100 million percent
    (of the cell size) for the spot size...
  \item The \code{fant} ground truth image is used as the "ideal scenario" for a
    downsampled image, as the algorithm is
    ``complete in the sense that all the pixels of the input 
    image under the map of the output image fully contribute to the output image.''
    as said by Karl Fant. This image is used to ensure a fairer comparison
    and that any image resizing artifacts that we have introduced do not
    significantly affect the order/trend of the scores. In preliminary tests,
    the trend was observed to be largely preserved, but with the scores
    being significantly higher overall.
  \item Manually downsampling the ground truth image using different algorithms
    in an image editor, artifacts such ``edge halos` (for ``Bilinear'' and ``Lanczos'')
    and ``edge loss'' (for ``Nearest Neighbor'') were observed.
    ``Bicubic'' performed relatively well, but had blurrier edges than using ``Fant''.
    These images are provided with the source code, along with
    instructions (on how they were produced).
\end{enumerate}


How test will be performed: Running the javascript and python tests.
Executing this suite of tests will compare the ground truth image to each
test image (see \ttref{Tbl_TestImages}) using all the different metrics
(\lref{lib_sewar} to \lref{lib_msssim}).


\item{\textbf{U\refstepcounter{unittestnum}\theunittestnum \label{U_metric8}}:
Control testing}

Type: Automatic

Input: The ground truth images (\code{og} and \code{fant}) compared to each of
the control test images (as listed in Table \ref{Tbl_TestImages}).
Except for using all the other metrics
(\lref{lib_sewar} to \lref{lib_msssim}, except for \lref{lib_prog}),
these are the same control tests as described earlier
(\utref{U_metric0A} to \utref{U_metric6}).

Output:
\begin{itemize}
  \item \utref{U_metric0A} Test Variant: same output and justification.
  \item \utref{U_metric0B} Test Variant: same output and justification.
  \item \utref{U_metric1} Test Variant: same output and justification.
  \item \utref{U_metric2} Test Variant: a lower score than the base variant,
    since ``structural similarity'' is taken into account.
  \item \utref{U_metric3} Test Variant: same as above.
  \item \utref{U_metric4} Test Variant: same output and justification,
    since the compared images have the same (``flat'') structure.
  \item \utref{U_metric5} Test Variant: same as above.
  \item \utref{U_metric6} Test Variant: a lower score than the base variant,
    since ``structural similarity'' is taken into account.
\end{itemize}

How test will be performed: Running the javascript and python tests.
Executing this suite of tests will compare the ``control images''
to each over other (as described in \utref{U_metric0A} to \utref{U_metric6}),
but using the different metrics
(\lref{lib_sewar}, \lref{lib_ssim}, \lref{lib_mse},
\lref{lib_ssim1}, and \lref{lib_msssim}).

\end{itemize}



\newpage
\clearpage

\section{Traceability Between Test Cases and \\ Requirements}
Traceability matrices are used to simplify the process of identifying what needs to be changed 
if a component is modified. An ``X'' is used to indicate links between items in the table. 
When a component is changed, the elements marked with an ``X'' might need to be updated as well.

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    & R1
    & R2
    & R3
    & R4
    & R5
    & R6
    & R7
    & NFR1
    & NFR2
    & NFR3
    & NFR4
  \\ \hline
  \tref{T_inputImage}           &X& & & & & & & & & & \\ \hline
  \tref{T_exportImage}          & & & & & &X& & & & & \\ \hline
  \tref{T_spotSize}             & &X& & &X& & & & & & \\ \hline
  \tref{T_spotSize10}           & &X& & &X& & & & & & \\ \hline
  \tref{T_spotSize500}          & &X& & &X& & & & & & \\ \hline
  \tref{T_spotRotation}         & &X& & &X& & & & & & \\ \hline
  \tref{T_rasterGrid}           & & &X& &X& & & & & & \\ \hline
  \tref{T_subregion}            & & & &X&X& & & & & & \\ \hline
  \tref{T_reproduceGT}          &X&X&X& & &X& & & & & \\ \hline
  \tref{T_manualMetricHigh}     & & & & & & &X& & & & \\ \hline
  \tref{T_manualMetricLow}      & & & & & & &X& & & & \\ \hline
  \tref{T_manualMetricLow2}     & & & & & & &X& & & & \\ \hline
  \tref{T_manualMetricControl}  & & & & & & &X& & & & \\ \hline
  \tref{T_surveyUsability}      & & & & & & & & &X& & \\ \hline
  \tref{T_surveyMetric}         & & & & & & & &X& & & \\ \hline
  \tref{T_linters}              & & & & & & & & & &X& \\ \hline
  \tref{T_codeReview}           & & & & & & & & & &X& \\ \hline
  \tref{T_portability}          & & & & & & & & & & &X\\ \hline
  \utref{U_metric1} to \utref{U_metric8} & & & & & & &X&X& & & \\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between the Tests and Requirements}
  \label{Table:A_trace}
\end{table}

\newpage
\clearpage

\bibliographystyle{plainnat}
\bibliography{../../refs/References,../../refs/cas741,
../../refs/algorithms,../../refs/programming}

\newpage

\section{Appendix} \label{sec_Appendix}

This section contains the \textit{usability} and image quality (metric \textit{accuracy}) 
user survey questions, as well as a code review checklist.

\subsection{Usability Survey Questions} \label{survey_usability}

\begin{itemize}
  \item{What web browser do you use and what version is it?}
  \item{What operating system do you use?}
  \item{What is your screen resolution?}
  \item{Are you using a mouse and keyboard or a touchscreen?}
  \item{How much time (in seconds or minutes) did it take you to find the function you were looking for?}
  \item{What was the most useful feature for you?}
  \item{On a scale from 1 to 10, how easy was it to set up and start the software?}
  \item{On a scale from 1 to 10, how easy was it to use the software?}
  \item{What was the least enjoyable feature to use?}
  \item{Was everything legible and clearly displayed? If not, what was not?}
  \item{Would you consider using the software as a learning or teaching tool?}
  \item{Do you have any extra comments or thoughts you'd like to share?}
\end{itemize}

\noindent Alternatively, the ``Quick and Dirty Usability Scale'' \cite{qnd_usability} may be used
instead.

\subsection{Image Quality Survey Questions} \label{survey_metric}
Given multiple images that are numbered and a ground truth image, 
the user shall answer the following questions?

\begin{itemize}
  \item{Which image looks to be the clearest in your opinion?}
  \item{Which image looks to be the highest quality in your opinion?}
  \item{Which image provide the most information or detail?}
  \item{Which image is the most similar to or the most representative of the ground truth image?}
  \item{Which image is the least pleasant in your opinion?}
  \item{Rank the images from the lowest quality (least clear) to the highest quality (most clear) in your opinion?}
  \item{Rank the images from least to most informative (or has most of the details of the ground truth image)?}
  \item{The user is shown the ground truth image and the reproduced control image 
  (generated using the same image resolution and exact-sampling). 
  Do these images look identical to you? If not, on a scale of 1 to 10, how similar are these images?}
\end{itemize}

\subsection{Code Review Checklist} \label{checklist_codeReview}

\begin{itemize}
  \item{Does the code follow a consistent style?}
  \item{Are there any variables that should not be global?}
  \item{Are function names not too long (i.e. less than 40 characters)?}
  \item{Are most function well named and clear in what they do?}
  \item{Are there any long lines of that can be split into multiple lines?}
  \item{Are global variables and functions documented?}
  \item{Is each function not too long and sufficiently broken down to accomplish a single task?}
  \item{Are comments plentiful, but not noisy?}
  \item{Is the code sorted into separate files where reasonable?}
  \item{Is there any duplicate that code be avoided?}
  \item{Is there any use of jargon, domain-specific, or unclear terms?}
  \item{Is all of the code reachable?}
  \item{Is the control flow convoluted?}
  \item{Is there any unnecessarily obscure code? If necessary, is it commented and explained?}
  \item{Could programmer other than the author read the code and briefly understand it?}
  \item{Is there any leftover commented code that is not useful?}
\end{itemize}

\subsection{Code Linting Ruleset} \label{codelint_ruleset}
The code linting ruleset should be the default recommended ruleset plus the following:
\begin{itemize}
  \item No lines longer than 120 characters.
  \item No function or variable names are at least 2 characters long and at most 40 characters long.
    Exceptions:
    \begin{itemize}
      \item \code{i}, \code{j}, \code{k}: common for iterators in loops
      \item \code{x}, \code{y}: common for positioning and coordinates
      \item \code{w}, \code{h}: common for width and height values
      \item (ESLint only) \code{e}: \textit{defacto} standard for javascript events
      \item \code{c}, \code{d}: used heavily in \progname{}'s image data manipulation.
        Otherwise, the code gets very difficult to read when dealing multiple arrays of data 
        on a lower level
    \end{itemize}
  \item No "magic numbers". Exceptions:
    \begin{itemize}
      \item \code{0}, \code{1}: common for array starting indexes
      \item \code{2}: common for getting half of a value
      \item \code{3}, \code{4}: used heavily for managing RGBA values/data on a lower level
      \item \code{-1}: common value for failure or an error
      \item \code{100}: common for percentages
      \item \code{180}: common for angle conversions
      \item \code{255}: the maximum value for an 8-bit pixel value.
        \progname{} is limited to 8-bit images as specified in the SRS \cite{SRS}
        and is unlikely to change as specified in the MG \cite{MG}.
    \end{itemize}
  \item Warn about global variables. Each shall be carefully considered whether it should remain
    a global variable. Library names are exempt.
  \item (flake8 only): ignore indentation (tabs vs. spaces) style \code{--ignore=W191}.
\end{itemize}


\subsection{Symbolic Constants} \label{secsymbolic_constants}
These are \textit{symbolic} values used in the descriptions and text in this document that are 
subject to change. They listed and defined here to make it easy to change and reduce the
possibility of mistakes when updating this document.
\begin{itemize}
  \item \code{SETUP\_MAX\_CLICKS = 4} clicks
  \item \code{SETUP\_MAX\_TIME = 3} minutes
  \item \code{ERR\_MARGIN = 0.02} (2\%)
\end{itemize}



\end{document}