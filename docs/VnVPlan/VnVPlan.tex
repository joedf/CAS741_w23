\documentclass[12pt, titlepage]{article}

\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
% \usepackage[round]{natbib}
\usepackage[numbers,square]{natbib}

\newcounter{testnum} %test Number
\newcommand{\dthetestnum}{T\thetestnum}
\newcommand{\tref}[1]{T\ref{#1}}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2023/02/17 & 1.0 & First version \\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  NFR & Nonfunctional Requirement\\
  GUI & Graphical User Interface\\
  R & Requirement\\
  ROI & Region of Interest\\
  SRS & Software Requirements Specification\\
  T & Test\\
  UI & User Interface\\
  VnV & Verification and Validation\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}

This document lays out the Verification and Validation (VnV) plan of the \progname{} (scanning electron 
microscope image formation) as described by the Software Requirements Specification (SRS) 
document \citep{SRS}. Testing of the software and its components is conducted to build confidence in 
its usability, accuracy, and ultimately whether it meets the SRS.

\section{General Information}

\subsection{Summary}

The \progname{} software aims to be a demonstration and learning tool of how a scanning electron 
microscope (SEM) formulates an image. The idea is to help visualize the influence trends of imaging 
parameters (as defined in the SRS) on image quality (or clarity) and resolution.


\subsection{Objectives} \label{sec_objectives}

The objective is to produce a tool that is easy to use (\textit{usability}). It should feel intuitive 
to the user and should provide easy to understand information. This is so that the user may 
identify any trends in how the final image is affected with relative ease and confidence. As a 
secondary goal, the \textit{accuracy} in the trends are important. As such, the image metrics 
should be straight-forward to understand and provide some kind of assurance away from the 
subjective nature of image quality.

\subsection{Relevant Documentation}

There are multiple design documents that provide the important and intimate details to understand 
some of the concepts that are being tested. These documents are the following:

\begin{itemize}
  \item Problem Statement \citep{Prob_Statement}
  \item SRS \citep{SRS}
  \item VnV Report \citep{VnV_report}
\end{itemize}

\section{Plan}

In this section, multiple plans are described to test and inspect the software with an emphasis 
on \textit{usability}. Multiple approaches and perspectives will be employed by the VnV team (\ref{sec_vnv_team})
to help build confidence in the requirements, to avoid any missed important details, 
and to deliver on the qualities as mentioned in the objective (\ref{sec_objectives}).

\subsection{Verification and Validation Team} \label{sec_vnv_team}

The members of the VnV team as well their individual roles are listed in the following table:

\begin{table}[h!]
  \centering
  \begin{tabular}{|r|l|}
    \rowcolor[gray]{0.9}
    \hline
    \textbf{Role} & \textbf{Name} \\ \hline
    Project Supervisor & Dr.\ Spencer Smith  \\ \hline
    Author             & \authname           \\ \hline
    Domain Expert      & Karen Wang          \\ \hline
    SRS Reviewer       & Jason Balaci        \\ \hline
    VnV Plan Reviewer  & Sam Crawford        \\ \hline
    MG + MIS Reviewer  & Lesley Wheat        \\ \hline
    Expert Consultant  & Dr. Nabil Bassim    \\ \hline
    Expert Consultant  & Michael W. Phaneuf  \\ \hline
  \end{tabular}
  \caption{Table of the VnV Team Members}
  \label{table_vnv_team}
\end{table}

\an{Dr. Bassim is my PhD Supervisor, and Mr. Phaneuf is my co-supervisor (employer/industry), 
since I am doing an industrial PhD. Much of this project was originally conceptualized with 
the help and guidance of Mr. Phaneuf.}


\subsection{SRS Verification Plan}

The SRS document is reviewed by the project supervisor, the SRS reviewer and the author. Some input
may be given by the expert consultants if time permits. Most of the feedback has been provided 
as issues on GitHub, or as annotated documents, or by verbal exchange. The author is then expected
make the resolve the issues and makes accordingly throughout the development of the project.
The key objectives are to verify that the software requirements and the documentation are coherent
and sound to the intended audience as defined in the SRS.


\subsection{Design Verification Plan}

Much of the conceptualization was done after having multiple discussions with the expert 
consultants and having done literature review for preexisting tools and documentation 
relevant to the project. Decision are made with focus on the usability, with little to 
no setup being required. The design and implementation is documented in the 
MG / MIS \citep{MG,MIS}.
The VnV team as well as any volunteer users are welcome to provide
their input (through GitHub issues). Eventually, the project may be published in a journal
where the software \progname~and its accompanying documentation will 
likely be rigorously reviewed.

\an{Not sure what was appropriate to put here.}


\subsection{Verification and Validation Plan Verification Plan}

The goal is to uncover any mistakes and reveal any risks through the supervision and 
review of the VnV team members. Once most of the work has been done, the work and
accompanying documentation shall undergo a final vetting process. Mainly, the team
will check whether the documented testing plans and verification process have been 
accomplished and the requirements fulfilled.


\subsection{Implementation Verification Plan}

Much of the software will be tested manually by users. This will include checking for
any inconsistencies, bugs in the graphical user interface (GUI), and any unexpected
artifacts in images produced.
The image metrics will be tested using unit tests (section \ref{sec_unittest}). For the
all the code implemented, linters will be used as mentioned in section \ref{sec_autotest_tools}.
As a control for the image metrics, they will be calculated using the same image as the ground truth 
reference to rule out any baseline or unexpected factors in the implementations themselves.


\subsection{Automated Testing and Verification Tools} \label{sec_autotest_tools}

The image quality metric shall be unit-tested using \href{https://pytest.org}{pytest} for 
automated testing of any algorithms implemented in Python and \href{https://qunitjs.com}{QUnit} 
shall be used for those implemented in JavaScript. The unit tests are listed in 
section \ref{sec_unittest}.
As for linter, \href{https://flake8.pycqa.org}{flake8} shall be used for Python code 
and \href{https://eslint.org}{ESLint} for JavaScript code.
The \href{https://github.com/andrewekhalel/sewar}{sewar} python package will be 
used as a reference implementation in Python for the image quality metrics.


\subsection{Software Validation Plan}
A \textit{usability} survey will be conducted to evaluate the user experience and whether 
the GUI is intuitive enough to the intended users as described in the SRS \citep{SRS}.
An \textit{accuracy} survey will be conducted to assess the user-perceived image quality. 
The trends identified in the surveys results will be compared to the calculated image metrics.
As a control for the images produced, a manual and an automated test will be conducted to verify if 
an image identical (or with unperceivable difference) to ground truth image can be reproduced.
The compared images shall be in the \textit{accuracy} survey for confirmation. This can be compared
as well using the image metrics. 

\an{I think I might be confusing the Software and the Implementation validation plans...}

\section{System Test Description}

In this section, the system tests that will be conducted as described in detail. These tests
will be used to verify the fulfillment of the requirements as listed in the SRS \citep{SRS}.
Most, if not all, of the tests listed here will be manually performed. Automatic
tests will be unit tests as described in section \ref{sec_unittest}.

\subsection{Tests for Functional Requirements}

The tests present will verify whether the functional requirements 
(as listed in the SRS \cite{SRS}) are met and validate that the outputs
are as expected.

\subsubsection{Image Import and Export}

To satisfy R1 from the SRS \citep{SRS}, an input image for the follow formats shall be 
accepted provided they follow the input data constraints.
Some preloaded example images shall be included and be available to the user for use as well.
As well as for R6, the software should allow for export of the resulting image.

\begin{itemize}
  \item{PNG}
  \item{JPG}
  \item{BMP\\}
\end{itemize}

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_inputImage}}: Test import for PNG/JPG/BMP format\\}
            
  Control: Manual
            
  Initial State: Loaded and idle.
            
  Input: A non-corrupt (valid) PNG, JPG, or BMP image file.
            
  Output: The image should be visible and be displayed as expected as the ground truth image.
            
  Test Case Derivation: Theses are some of the most common image file formats and should be compatible with the software.
            
  How test will be performed: The user will click the "Load image" button and select an image to import.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_exportImage}}: Test PNG Image Export\\}
              
  Control: Manual
            
  Initial State: Loaded and idle.
            
  Input: not needed, the software defaults to a preloaded image if none is given.
            
  % Output: \wss{The expected result for the given inputs}
  Output: The output image file should look the same as what is displayed in the "Resulting Image" display.

  % Test Case Derivation: \wss{Justify the expected value given in the Output field}
  Test Case Derivation: \an{Not sure what this means, and how this differs from what's explained Output.}

  How test will be performed: The user will click the "Export image" button and choose where to save the image file.

\end{enumerate}


\subsubsection{Spot Profile and Imaging Parameters}

To satisfy R2 and R5 from the SRS \citep{SRS}, the software should accept input from the user
to change the width, height, and rotation which define an ellipsoid shape. 
This shape is then used as the spot to sample the ground truth image.
To satisfy R3, the software should accept user input for a real positive number for the pixel size.
TO satisfy R4, the user should be able to specify a subregion (or ROI) for processing.

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_spotSize}}: Spot Width and Height\\}

    Control: Manual
              
    Initial State: Running and idle.
              
    Input: Width and Height as positive real numbers relative.
              
    Output: The spot layout should reflect these changes display an updated arrangement with the given shape.
    The resulting image should exhibit expected effects as document the SRS figures \citep{SRS}.

    Test Case Derivation: \an{not needed in this case?}
              
    How test will be performed: Either by scrolling in the visual spot shape UI or by number input in the GUI.
					
  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_spotRotation}}: Spot Rotation\\}

    Control: Manual
                
    Initial State: Running and idle.
              
    Input: An angle in degrees as a real number.
              
    Output: The spot layout should reflect the updated arrangement with the rotated spots.
    The resulting image should exhibit expected effects as document the SRS figures \citep{SRS}.

    Test Case Derivation: \an{see output?}

    How test will be performed: By dragging the rotation node in the visual spot shape UI.
					
    \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_rasterGrid}}: Raster Grid / Pixel Size\\}

    Control: Manual
                
    Initial State: Running and idle.
              
    Input: Positive real numbers for the rows and columns used for the raster grid to calculate the number of pixels and their size.
              
    Output: The resulting image resolution should grow or shrink according to the pixel size given.

    Test Case Derivation: \an{see output?}

    How test will be performed: The user shall input the number of rows and columns.

\end{enumerate}


\subsubsection{Image Quality Metric}

To satisfy R7, the user be able to see a larger number for a resulting image that is closer to ground truth, 
and a smaller number otherwise.

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_manualMetricHigh}}: Higher metric value\\}

    Control: Manual
              
    Initial State: Running and idle.
              
    Input: Spot size between 100\% to 130\%.
              
    Output: A number that is higher than if the spot size were outside the 100\% to 130\% range.

    Test Case Derivation: The resulting image should look somewhat sharp.
					
  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_manualMetricLow}}: Low metric value\\}

  Control: Manual
              
  Initial State: Running and idle.
            
  Input: Spot size from 0\% to 90\% or from 140\% or more.
            
  Output: A number that is lower than if the spot size were inside the range of 100\% to 130\%.

  Test Case Derivation: The resulting image should look somewhat blurry or overly sharp or pixelated.

\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

The follow tests will check if the nonfunctional requirements as defined in the SRS \citep{SRS} are 
met. The emphasis is on the \textit{usability} (NFR2) of the software. In the case of \progname~
simplicity is more important than the actual complexity or correctness.
To satisfy the \textit{accuracy} (NFR1) requirement, an image quality and clarity survey shall be
conducted to establish what is perceived by the user and the image metric of what 
a "better" image is. To satisfy the \textit{maintainability} (NFR3) requirement, the code should 
follow a consistent style and reasonably pass static code analysis as described below.
As for \textit{Portability} (NFR4), the user should be run the software on their platform and 
environment of choice.

\subsubsection{Usability Testing}
\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_surveyUsability}}: Usability Survey\\}

  Type: Manual
            
  Initial State: None
            
  Input/Condition: A group of intended users, the SRS document \citep{SRS}, 
  the survey questions on topics related to \textit{usability}.
            
  Output/Result: The completed user surveys (see \ref{survey_usability}).
            
  How test will be performed: The users will be given a series of questions to evaluate the 
  ease-of-use, whether the GUI is intuitive or confusing, etc.

\end{enumerate}

\subsubsection{Accuracy Testing}
\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_surveyMetric}}: Image Metric Survey\\}

  Type: Manual
            
  Initial State: None
            
  Input/Condition: A group of intended users, the SRS document \citep{SRS}, 
  the survey questions on topics related to \textit{usability}.
            
  Output/Result: The completed user surveys (see \ref{survey_usability}).
            
  How test will be performed: The users will be given a series of questions to evaluate the 
  ease-of-use, whether the GUI is intuitive or confusing, etc.
  
\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description} \label{sec_unittest}

\wss{Reference your MIS (detailed design document) and explain your overall
  philosophy for test case selection.}  
\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    & R1
    & R2
    & R3
    & R4
    & R5
    & R6
    & R7
    & NFR1
    & NFR2
    & NFR3
    & NFR4
  \\ \hline
  \tref{T_inputImage}        & & & & & & & & & & & \\ \hline
  \tref{T_exportImage}       & & & & & & & & & & & \\ \hline
  \tref{T_spotSize}          & & & & & & & & & & & \\ \hline
  \tref{T_spotRotation}      & & & & & & & & & & & \\ \hline
  \tref{T_rasterGrid}        & & & & & & & & & & & \\ \hline
  \tref{T_manualMetricHigh}  & & & & & & & & & & & \\ \hline
  \tref{T_manualMetricLow}   & & & & & & & & & & & \\ \hline
  \tref{T_surveyUsability}   & & & & & & & & & & & \\ \hline
  \tref{T_surveyMetric}      & & & & & & & & & & & \\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between the Tests and Requirements}
  \label{Table:A_trace}
\end{table}


\newpage
\clearpage

\bibliographystyle{plainnat}
\bibliography{../../refs/References,../../refs/cas741}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?} \label{survey_usability}

\wss{This is a section that would be appropriate for some projects.}

\end{document}